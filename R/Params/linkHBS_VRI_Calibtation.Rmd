---
title: "Estimating uncertainty in forest inventory projections"
output: 
  html_document: 
    keep_md: yes
---

<!--
Copyright 2018 Province of British Columbia
 
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
 
http://www.apache.org/licenses/LICENSE-2.0
 
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.-->


# Introduction

Estimates of timber volume projected through time and across landscapes are need to assess the sustainability of forestry decisions. The economic value of timber and many ecological processes of interest to decision makers (e.g., biomass, carbon, wildlife habitat) are all driven by projections of forest structure which are often characterized by timber volume. A common concern by forestry decision makers is the level of uncertainty behind these projections across large landscapes. 

There are many sources of uncertainty in timber volume projections and these sources propagate through time in complex ways which may provide barriers for confidentaly making forest management decisions. Past research has highlighted how sources of uncertainty related to errors in forest attributes reported in the forest inventory would propogate with errors in growth and yield models and thus, impact harvest scheduling. Recently, Robinson et al. (2016) recommended a simple way to incorpate this uncertainty into forestry decision making. Their approach relied on observed timber volumes measured from scaling and cruising operations following harvesting. In particular, the value of the Robinson et al. (2016) approach is two fold: i.) a calibration of the future timber volumes for use when simulating the forestry decisions and ii.) the ability to simulate the uncertainty of timber volume projection and thus calculate error statistics useful to decision makers. Robinson et al. (2016) modelled the distributional parameters of a prediction by specifying a seperate model for the mean and scale. These parameters were then used to reconstruct the response distribution for an individual harvest unit which can be summed to arrive at a distribution for the total volume harvested. Thereby allowing the construction of a prediction interval on the total volume harvested.

An important assumption of this approach is that the individual harvest units need to be independant of one another, in order to be able to sum the individual distributions. While this assumption was not formally tested, the authors highlight that if the individual harvest units were positively correlated then they would jointly contribute more to the uncertainty than their summation would suggest and conversely contribute less to uncertainty, if they were negatively correlated. This is an important point that could be leverage to develop harvest schedules that result in reduced uncertainty.

Currently, this approach has not been applied in the province of BC given issues with spatially tracking observed scaled volumes. The tracking of observed scaled volume is accomplished via the harvest billing system (HBS) which focuses on harvest volumes, needed for provincal revenue accounting, at a spatial unit known as a timber mark. The timber mark is a unique identifer stamped or marked on the end of each log to associate the log with the specific authority to harvest and move timber. Timber marks are assigned to harvested units that may be not be spatially contiguous nor contain a single harvest date. Thus, linking HBS and forest inventory data has been difficult due to issues with temporal and spatial accuracy in estimating the spatial boundaries of the timber marks. In particular, estimating the net area harvested (i.e., netting out roads, wildlife tree patches, etc) and its spatial boundary can be difficult to determine for historical cutblocks.

In the following sections, the spatial (timber mark boundaries) and temporal (harvest date, inventory projection) data are manipulated to develop a calibration model that can be used to provide valuable information about timber projection uncertainty. It is believed that this calibration will support adaptive forest management by enhancing the link between strategic decisions and their implementation. The specific objectives were: i) to calibrate projected volume yields from an aggregated growth and yield model (hereafter termed the meta-model) with observed volume yields reported in the harvest billing system and ii) determine how uncertainty in total harvested volumes can be minimized. The proposed calibration model will be used in the caribou and landuse model (CLUS) to provide an estimate of volume yield uncertainty which will help provide some context surrounding the quantification of impacts from caribou conservation activities on harvest flows. 

# Methods
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(sf)
library(dplyr)
library(ggplot2)
library(Hmisc)
library(bcmaps)
library(gstat)
library(ape)
library(ade4)
library(plotly)
library(gamlss)
library(cowplot)
library(ncf)
source("C:/Users/KLOCHHEA/clus/R/functions/R_Postgres.R")
```

## Linking timber mark boundaries with VRI

Linking the timber mark boundaries with the VRI (needed to link to a growth and yield model) involved the following: (see [code](https://github.com/bcgov/clus/blob/master/SQL/develop_vri2011table.sql))

1. Subset and estimate the spatial boundaries of the timber marks

The spatial boundaries of the timber mark were estimated using the compilation of two spatial data sets: i) [forest tenure cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/forest-tenure-cutblock-polygons-fta-4-0) (aka. ftn_c_b_pl_polygon) and ii) [consolidated cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/harvested-areas-of-bc-consolidated-cutblocks-) (a.k.a cns_cut_bl_polygon). While the forest tenure polygons provided the spatial boundary of the timber mark, these boundaries include retention areas which can be quite large. Thus, the consolidate cutblock polygons dataset was used to remove these non-harvested areas within a timber mark area.

Using the forest tenure cutblock polygons, all timber marks comprised of harvest units that were spatially contiguous and had disturbance start dates between 2012-01-01 and 2016-12-31 were selected. This ensured timber marks would be roughly at the scale of a harvest unit or cutblock and would temporally link to the forest inventory. Further, timber marks with scaled dead wood were removed since the naive projections do not include standing dead timber. Lastly, timber marks were removed if they did not contained geometries reported by RESULTS (the most accurate form of reporting cutblock information). This ensured a more accurate net spatial description of the timber mark boundary (i.e., retention patches were removed). Despite the accuracy of the RESULTS, there remain a number of issues with identifying non-harvest areas in the consolidated cutblock polygons. Thus, timber marks were removed when the estimated timber mark area was not within $\pm$ 20% of the planned net area.This subset of the forest tenure cutblock polygons was then joined with consolidated cutblock polygons to provide a spatial boundary that exluded non harvest areas. 

2. Intersect the timber mark spatial boundaries with the VRI

The resulting spatial boundaries that excluded non-harvested areas were then spatialy intersected with the 2011 VRI to provide the necessary forest attribute information require to link to the growth and yield model. Only the dominant layer (layer rank 1) of the forest inventory was used, thereby simplifying the growth and yield proejction. As a result of this intersection, 1264 of the timber marks had a portion of their total area that failed to provide the neccessary forest inventory information (i.e., areas that lack the publically available forest inventory such as Tree Farm Licenses). 

The following is a histogram of the percentage of the total timber mark area missing information:

```{r, timber_mrks, echo = FALSE, fig.cap = 'FIGURE 1. A histogram of the percentage of the total timber mark area that did not match with forested VRI polygons. q25 and q75 are the 25th and 75th quantile, respectively.'}
feats<-getSpatialQuery("SELECT feature_id, shape, yc_grp, bec_zone_code, site_index, crown_closure, pcnt_dead, proj_height_1, proj_age_1 FROM yt_vri2011")
#cnx_hbs<-getSpatialQuery("SELECT * FROM cnx_hbs")

cnx_hbs<-getSpatialQuery("SELECT * FROM cnx_hbs where timber_mrk in (select timber_mrk from (
select count(*) as no_opens, foo.timber_mrk from (select count(*) as no_opening_id, opening_id, timber_mrk from cnx_hbs 
group by opening_id, timber_mrk order by timber_mrk) as foo
group by foo.timber_mrk) as foo2 where no_opens = 1)")

cnx_hbs<-lwgeom::st_make_valid(cnx_hbs)

polys<- st_intersection(cnx_hbs, feats) #spatialy join the timber mark boundaries with the id that links the VRI
polys$area<-st_area(polys)/10000 #calc the area of the individual polygons within a timber mark
polys$time_since_inv<-polys$harvestyr -2011
polys$age_hrv<-polys$proj_age_1 + polys$time_since_inv


#get elevation?
ras.elv<-getTableQuery("SELECT  timber_mrk, (stats).*
FROM (SELECT timber_mrk, ST_SummaryStats(ST_Clip(rast, feat.wkb_geometry)) As stats
      FROM rast.dem
      INNER JOIN  (SELECT timber_mrk, st_union(wkb_geometry) as wkb_geometry 
				   FROM cnx_hbs where timber_mrk in (select timber_mrk from (
select count(*) as no_opens, timber_mrk from (select count(*) as no_opening_id, opening_id, timber_mrk from cnx_hbs 
group by opening_id, timber_mrk order by timber_mrk) as foo
group by timber_mrk) as foo2 where no_opens = 1   ) group by timber_mrk
   ) as feat
    ON ST_Intersects(feat.wkb_geometry,rast) 
 ) As foo")
    
#Create a temp table?
conn<-GetPostgresConn()
dbWriteTable(conn, "polys",polys)
dbDisconnect(conn)

yt_prj<-getTableQuery("SELECT t.feature_id, t.yc_grp, t.bec_zone_code,
  (((k.prj_vol_dwb - y.prj_vol_dwb*1.0)/10)*(t.age_hrv - CAST(t.age_hrv/10 AS INT)*10))+ y.prj_vol_dwb as itvol
  FROM polys t
  LEFT JOIN vdyp_test3 y 
  ON t.yc_grp = y.yc_grp AND CAST(t.age_hrv/10 AS INT)*10 = y.prj_total_age
  LEFT JOIN vdyp_test3 k 
  ON t.yc_grp = k.yc_grp AND round(t.age_hrv/10+0.5)*10 = k.prj_total_age WHERE t.age_hrv > 0
  ;")

#delete the temp table
conn<-GetPostgresConn()
dbExecute(conn, "Drop table polys;")
dbDisconnect(conn)

#join with poly
out.tbl<-merge(polys, yt_prj)
out.tbl$vol<-out.tbl$area*out.tbl$itvol #calc the volumes for each of the individual polygons within a timber mark
#st_write(out.tbl, "test3.shp")

#get the itvols that are NULL == don't match with the meta-model
out.tbl2<-data.table(out.tbl)
units(out.tbl2$area) <- NULL
units(out.tbl2$vol) <- NULL

#43118
out.tbl2<-out.tbl2[!(timber_mrk == 'BY5H37'),] #this timber mark also has a missing portion 

test.1 <-data.table(out.tbl2)
number_obs<-test.1[, .(count = .N, var = sum(area)), by = timber_mrk ]

#figure out the distrubution of area with no matching
noMatchArea<-out.tbl2[is.na(itvol), sum(area), by =timber_mrk]
setnames(noMatchArea, "V1", "area")
totalArea<-out.tbl2[, sum(area), by =timber_mrk]
setnames(totalArea, "V1", "totarea")
percentNoMatch<-merge(noMatchArea, totalArea)
percentNoMatch$pernomatch<-percentNoMatch$area/percentNoMatch$totarea


#plot the distribution
eq <- substitute(italic(q25)== a*","~~italic(q75)== b ,list(a=as.numeric(quantile(percentNoMatch$pernomatch,0.25)), b= as.numeric(quantile(percentNoMatch$pernomatch,0.75))))
  
ggplot(data = data.table(per.Area.Missing=percentNoMatch$pernomatch), aes(x=per.Area.Missing)) +
  geom_histogram(bins =120)+
  geom_vline(xintercept=quantile(percentNoMatch$pernomatch,0.25))+
  geom_vline(xintercept=quantile(percentNoMatch$pernomatch,0.75))+
  geom_text(aes(x = 0.30, y =400 , label = as.character(as.expression(eq)) ), parse = TRUE) + 
  theme_bw()

```

This histogram shows that 75% of timber marks were missing forest inventory attribution for only 1.4% of their total area. After visually checking, two issues arose: i) the spatial boundaries of these timber marks extend into non-forested area as reported by the VRI; and ii) there wasn't enough information to parameterize the growth and yield model (outside the domain of the inputs, e.g., recently disturbed). In the case of the non-forested areas, these could be small spatial errors in the VRI and were thus important sources of uncertainty. However, from a practical view, these relatively small areas would contribute little to the total projected volume estimate. Thus, timber marks with greater than 3 percent of their total area containing inadequate forest inventory information were removed from the analysis.


```{r, sample, echo = FALSE, fig.cap = 'FIGURE 2. The location of timber marks used in the analysis (n = 326).'}
noMatch2<-unique(percentNoMatch[pernomatch > 0.03, timber_mrk])
timbr_mrks<-out.tbl2[!(out.tbl2$timber_mrk %in% noMatch2),]
timbr_mrks[is.na(vol), vol:=0]
test.1 <-data.table(timbr_mrks)
number_obs<-test.1[, .(count = .N, var = sum(area)), by = timber_mrk ]

out.tbl3<-timbr_mrks[, sum(vol), by =timber_mrk]
setnames(out.tbl3, c("V1", "timber_mrk"), c("proj_vol","timber_mark"))

out.tbl3.area<-timbr_mrks[, sum(area), by =timber_mrk]
setnames(out.tbl3.area, c("V1", "timber_mrk"), c("area","timber_mark"))

out.tbl4<-merge(out.tbl3,out.tbl3.area)

hbs_obs<-getTableQuery("SELECT timber_mark, sum(volume_m3) as obs_vol FROM hbs_select_tmbr_mrk  where waste_type = 'Non-Waste' AND coast_interior = 'Interior' group by timber_mark")

dead_hbs_obs<-getTableQuery("SELECT distinct(timber_mark) FROM hbs_select_tmbr_mrk where grade = 'CB Dead'")
hbs_obs<-hbs_obs[!(hbs_obs$timber_mark %in% dead_hbs_obs$timber_mark),]

calb_data<-data.table(merge(hbs_obs, out.tbl4))
units(calb_data$area) <- NULL
units(calb_data$proj_vol) <- NULL

calb_data<-calb_data[proj_vol > 100 , ] # get rid of small blcoks that may not be some sort of alternative cutting system
calb_data<-calb_data[obs_vol > 100 , ] # get rid of small blcoks that may not be some sort of alternative cutting system

#check outliers
#'WAWVFF' ,'WBJKDD' , '88606', 'DE2270', '52/911', 'DE2621', 'EU6615'
#WAWVFF is a small triangle problem with mapping the area its 0.6 ha so very small remove
#WBJKDD is also less than 1 ha remove
#No reason to remove 88606 -- looks ok VRI reporting a site index of 10
#No reason to remove DE2270 -- looks ok VRI reporting a site index of 8
#52/911 area wrong remove
#No reason to remove - DE2621 -- VRI site index 10-12 (low)
# Nor reason to remove EU6615 -- VRI site index 8

calb_data<- calb_data[!(timber_mark %in% c('WAWVFF','WBJKDD', '52/911')),]
calb_data$proj_vol<-as.numeric(calb_data$proj_vol)

#Get the spatial X,Y coordinates
cents<-st_read(paste0(here::here(), "/R/Params/centroid_timber_mkrs2.shp"))
cents2<-st_coordinates(st_sf(cents))
cents2<-data.table(cents2)
cents2$timber_mark<-cents$tmbr_mr
calb_data<-merge(calb_data, cents2, by.x = "timber_mark", by.y ="timber_mark", all.x = TRUE)

#Get the elevation
ras.elv<-data.table(ras.elv)
setnames(ras.elv, "timber_mrk", "timber_mark")

ras.elv2<-ras.elv[, weighted.mean(mean, count), by =timber_mark]
ras.elv2[timber_mark == '90884', V1:= 1372]
calb_data<-merge(calb_data, ras.elv2, by.x ="timber_mark", by.y = "timber_mark", all.x = TRUE )
setnames(calb_data, "V1", "elv")
#map the observations?
bec <-get_layer("bec")
ggplot() + geom_sf(data = bec, aes(fill=ZONE), size =0.1)+ geom_point(data=calb_data, aes(x=X, y=Y), color="red")
```


## Growth and yield meta-model

In BC, forested stands from natural orgins are typically projected through time using [Variable Density Yield projection](https://www2.gov.bc.ca/gov/content/industry/forestry/managing-our-forest-resources/forest-inventory/growth-and-yield-modelling/variable-density-yield-projection-vdyp) (VDYP). VDYP is a stand-level empirical growth and yield model that uses VRI attribution as inputs into its sub-models. Using the 2018 vintage of the VRI, each polygon was projected for 350 years using 10 year time steps. The result of this process was a dataset with over 3.5 million yield curves which took 3 days to complete on a intel xeon, 3.5 ghz processor with 64 GB of RAM. Both the input (VRI information) and outputs (yields over time) were uploaded into a PostgreSQL database for further processing. Using the layer 1 rank information (the dominant layer),  yield curve groups (yc_grp) or anlaysis units were constructed using: BEC zone, site index (2 m interval), height class (5 classes as per the VRI) and crown closure class (5 classes as per the VRI). Each yc_grp was then aggregated by area weighting the respective individual polygon level yield curves. The result was a provincial database of composite yield curves that directly links to the VRI through the layer 1 rank attribution described above.

## HBS volumes vs projected meta-model volumes

Each VRI polygon that intersected the timber mark boundary and contained a projected volume was summed to estimate the total projected timber volume for the timber mark. This projected timber volume represented the naive projection commonly used in forest estate modeling.  

```{r, echo = FALSE, Step6_develop_vri2011, fig.cap = 'FIGURE 3. The relationship between observed (obs_vol) and projected (proj_vol) volumes ($m^3$). The yellow line is a one to one relationship; the blue line is a linear line of best fit; the dashed red lines represent the 95% prediction interval (n= 326).'}

model1 <- lm(obs_vol ~ proj_vol, data=calb_data)
summary(model1)
temp_var <- predict(model1, data =calb_data, interval="prediction")
calb_data2 <- cbind(calb_data, temp_var)

lm_eqn = function(m) {
  l <- list(a = format(as.numeric(coef(m)[1]), digits = 2),
      b = format(as.numeric(coef(m)[2]), digits = 2),
      r2 = format(summary(m)$r.squared, digits = 3))
  eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  as.character(as.expression(eq))                 
}

ggplot(calb_data2, aes(proj_vol, obs_vol)) +
  geom_point() +
  geom_smooth(method='lm', se = TRUE)+
  geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
  geom_line(aes(y=upr), color = "red", linetype = "dashed") +
  geom_text(aes(x = 35000, y = 100000, label = lm_eqn(model1)), parse = TRUE) +
    geom_abline(intercept =0, slope=1, col ="yellow") +
  theme_bw()


```

### Modelling assumptions

The following assumptions of the calibration model need to be tested: i) the response is gamma distributed, ii) the model fits the data and iii) the residuals are independant.

From the histogram below the gamma and log normal distributions appear to be candidates.

```{r, dist_proj_vol, fig.cap="FIGURE 4. Histogram of the projected volumes of timber marks between 2012 to 2016 (n=326)"}
#library(fitdistrplus)
#param.1 <- MASS::fitdistr(calb_data4$obs_vol, "normal")
#param.2 <- MASS::fitdistr(calb_data4$obs_vol, "lognormal")
#param.3 <- fitdistr(calb_data4$obs_vol, "gamma", start = list(shape = 1.2, scale = 0.0001), lower = 0.01)

hist(calb_data2$obs_vol, prob = T, ylim = c(0, 0.00007))
curve(dgamma(x, 1.2, 0.0001), add=TRUE, col = 'red')
curve(dnorm(x, 15138, 19838), add=TRUE)
curve(dlnorm(x, 8.79, 1.41), add=TRUE, col = 'blue')
```

## Calibration model

In Robinson et al. (2016) a gamma model was used to model both the  mean and  variance of the response distribution. Gamma models are advantageous because they are highly flexible in the positive domain and allow the modeling of heteroskedastic variance. Below we try a few gamma models by incorporating forest attributes as predictors of both the mean and variance. The results suggest a gamma model is better fit over a log normal model. Also, the VRI height is a important predictor of the response variance. However, these models assume no spatial correlation which needs to be tested.

```{r, calibrate_model,  fig.cap= 'FIGURE 4. Fit statistics of the calibration model (n=326)' }
# get the bec zone that makes up the majority of the timber mark 
pv_timber_mrk<-merge(timbr_mrks, totalArea)
pv_timber_mrk[,wt:=as.numeric(area/totarea)]
pv_timber_mrk2<-pv_timber_mrk[, lapply(.SD, function(x) {wtd.mean (x, wt)}), by =timber_mrk, .SDcols=c("proj_height_1", "site_index", "crown_closure", "proj_age_1", "pcnt_dead")]
setnames(pv_timber_mrk2, "timber_mrk" , "timber_mark" )
calb_data3<-merge(calb_data2, pv_timber_mrk2, by = "timber_mark")
calb_data4<-na.omit(calb_data3)

## Fit and compare some models
test.0 <- gamlss(obs_vol ~ proj_vol,
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.0 <- gamlss(obs_vol ~ proj_vol,
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

test.1 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ 1,
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)
test.1 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

test.2 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ proj_vol,
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.3 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ log(proj_vol),
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.4.ga <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ log(proj_vol) + proj_height_1,
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.4.ln <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ log(proj_vol) + proj_height_1,
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

LR.test(test.4.ln, test.4.ga)
chosen<-test.4.ga
summary(chosen)
plot(chosen)
```

### Testing for autocorrelation

An assumption of the uncertainty model is that the errors are independant - in order to be able to sum across many predictions and achieve the appropriate total. Here I test for autocorrelation in the residuals.

```{r, testing_auto_correlation}

ind.1<- predictAll(chosen, newdata = calb_data4)
ind.2<-cbind(calb_data4, ind.1$mu)
ind.2$res<-ind.2$obs_vol - ind.2$V2

#get distances
dists <- as.matrix(dist(cbind(ind.2$X, ind.2$Y)))
dists.inv <- 1/dists 
diag(dists.inv) <- 0
#dists[1:50, 1:50] # check what they look like - units are in metres

#auto.2$res<-auto.2$res+runif(326, 1, 10000)
Moran.I(ind.2$res, dists.inv)
#observed is significantly greater than expected  - positively correlated. Thus, jointly contribute more to the uncertainty then their sum would suggest.

xyspatial=SpatialPoints(cbind(ind.2$X,ind.2$Y))
porspatial=data.frame(ind.2$res)
spatialdata=SpatialPointsDataFrame(xyspatial,porspatial)

vario2 <- variogram(ind.2$res~1, spatialdata, cutoff = 3000)
plot(vario2)

bubble(spatialdata, "ind.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")


```

The results from the Moran's I suggest that the residuals are independant (Moran's I = 0.0239, p = 0.06556). After building a variogram there doesn't seem to be a strong spatial effect.  However, the bubble map shows some spatial clustering -namely in the far east corner and around the kamloops area. Also the p value for Moran's I is providing enough evidence against the null hypothesis to warrant a check to see if the model can be further improved by taking into account any spatial autocorrelation.

### Autocorrelated model

To account for the spatial autocorrelation in the model we construct a random effect with mean zero and covariance structure modelled using a spatial lag between observations. Thus, the off diagonal elements of the variance-covariance matrix will be estimated using a spatial covariance structure that is parameterized from a model assuming independance. The steps to fit this model included:

1.) Fit a gamma model for the mu and sigma.

2.) Use the residuals from this model to construct a spatially varying random variable that is normally distributed with mean 0 and covariance

3.) Specifiy the structure of this covariance matrix to follow a spatial lag that uses a spherical model of the X and Y locations

4.) Re-fit the gamma model with the spatially varying random variable

5.) Iterate through 1-4 until convergence criterion has been met

The result of this model is a Moran's I of 0.0235, p = 0.69, the bubble map shows less clustering, and the AIC of this spatial model is less (6162 versus 6170) than the model assuming independance. Thus, it appears the spatial model may be the prefered model. Lets see how well the predictions look. 

```{r, autocorr}
calb_data4$dummy<-1
chosen.a <- gamlss(obs_vol ~log(proj_vol) + re(random=~1|dummy, correlation = corSpher(2500, form = ~ X + Y, nugget = T), opt="optim", method = "REML"),
                   sigma.formula = ~ log(proj_vol) + proj_height_1 ,
                   sigma.link = "log",
                   family = GA(), data = calb_data4, control = gamlss.control(c.crit = 0.005), method=CG())
summary(chosen.a)


plot(chosen.a)

auto.1<- predictAll(chosen.reml, newdata = calb_data4)
auto.2<-cbind(calb_data4, auto.1$mu)
auto.2$res<-auto.2$obs_vol - auto.2$V2

Moran.I(auto.2$res, dists.inv)
#There is more evidence for the null hypothesis

xyspatial.auto=SpatialPoints(cbind(auto.2$X,auto.2$Y))
porspatial.auto=data.frame(auto.2$res)
spatialdata.auto=SpatialPointsDataFrame(xyspatial.auto,porspatial.auto)
vario2 <- variogram(auto.2$res~1, spatialdata, cutoff = 2500)
plot(vario2)

#look at the residuals -- less clustering going on
par(mfrow = c(2,1))
bubble(spatialdata.auto, "auto.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")
bubble(spatialdata, "ind.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")

chosen.reml$mu.coefSmo[[1]]$coefficients$random
#See how the coefficents change. Note the significance on the intercept. Intercept significance is different.
chosen.a$mu.coefSmo
summary(chosen.a)
summary(chosen)

#Note the AIC is much smaller than the model assuming independance
AIC(chosen.a)
AIC(chosen)

#Much residuals slightly larger with autocorrelated model -- more uncertainty that the indepdnant more would suggest
#sum(auto.2$res**2)-sum(ind.2$res**2)

```

### Comparing the sum of distributional parameters to simulating via monte carlo

In Robinson et al. (2016), each of the response distributions were summed after sampling 10000 samples from each of the response distributions. However, this is computationally slow ( adds ~ 30 seconds for 326 harvest units). Instead of sampling each individual response distribution -- we can sum the means and sigmas because the individual responses are independant. Here is a comparison between mathematically estimating the total response distribution versus simulating it with many samples.

```{r, iid, echo = FALSE}
#Assuming indepdnance model
test.iid.data<-ind.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1")] 
test.iid.0<- predictAll(chosen, newdata = test.iid.data) 
test.iid.data$mu<-test.iid.0$mu 
test.iid.data$sigma<-test.iid.0$sigma 
  
#sum(test.iid.data$obs_vol)
#sum(test.iid.data$proj_vol)
#sum(test.iid.data$mu)
#sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))

#est param
#sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))/sum(test.iid.data$mu)
summed<-rGA(20000, mu = sum(test.iid.data$mu), sigma = sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))/sum(test.iid.data$mu) )
#quantile(summed, c(0.05, 0.5, 0.95))

#-------
#Autocorrelated model
test.auto.data<-auto.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1", "dummy")] 
test.auto.0<- predictAll(chosen.a, newdata = test.auto.data) 
test.auto.data$mu<-test.auto.0$mu 
test.auto.data$sigma<-test.auto.0$sigma 
  
#sum(test.auto.data$obs_vol)
#sum(test.auto.data$proj_vol)
#sum(test.auto.data$mu)
#sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))

#-------

#simulate
sim.volume <-
    sapply(1:20000,
           function(x)
             with(test.auto.data,
                  sum(rGA(nrow(test.auto.data), 
                          mu = mu,
                          sigma = sigma))))

#hist(sim.volume)
#quantile(sim.volume, c(0.05, 0.5, 0.95))


#est param via math
#sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))/sum(test.auto.data$mu)
summed.auto<-rGA(20000, mu = sum(test.auto.data$mu), 
                 sigma = sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))/sum(test.auto.data$mu))
quantile(summed.auto, c(0.025, 0.05, 0.5, 0.95, 0.975))

# compare the distributiongs for math and sim
data.77<-rbind(data.table(vol = summed.auto, type = "math"), data.table(vol = sim.volume, type = "sim"))
ggplot(data.77, aes(vol, fill = type)) + 
geom_histogram(alpha = 0.5, position = 'identity') 

```

Now lets compare the model assuming independance and the spatial model. Here we see that the spatial model is less biased than the model that assumes independance. The mean of the spatial model is much closer to the observed total.

```{r, spat_mod_compare, echo = FALSE}



data.76<-rbind(data.table(vol = summed, type = "assume_iid"), data.table(vol = summed.auto, type = "spatial"))
ggplot(data.76, aes(vol, fill = type)) + 
geom_histogram(alpha = 0.5, position = 'identity') +
geom_vline(xintercept = sum(test.auto.data$obs_vol), size = 1.5 ) +
geom_text(aes(x=5250000, y=3500, label= "Observed"), hjust = 1)+
geom_vline(xintercept = sum(test.auto.data$proj_vol), size = 1.5, linetype = 'dotted') +
  geom_text(aes(x=6150000, y=3500, label= "Naively Projected"), hjust = 1)+
geom_vline(xintercept = sum(test.auto.data$mu), color = '#619CFF') +
geom_vline(xintercept = sum(test.iid.data$mu), color = '#F8766D') +
  labs(x = "Total Volume (m3)", y = "Count") +
  stat_function(fun = pnorm, n = 101, args = list(mean = 4935127, sd = 358751.2)) 


#mean(test.auto.data$obs_vol)*length(test.auto.data$obs_vol)
#sd.t<-((sd(test.auto.data$obs_vol)/sqrt(length(test.auto.data$obs_vol))))*length(test.auto.data$obs_vol)

#ci<-data.frame(lb=sum(test.auto.data$obs_vol) - (2*sd.t),ub= sum(test.auto.data$obs_vol) + (2*sd.t))


##Very close - essentially the same
```

### Compare obs vs predicted distributions

Here we compare the cummulative dsitributions to see any deviations between the predicted and response surfaces. They are very similar - a few deviations most notably at larger predictions of the response.
```{r, cdfs}
#ECDF
auto.2$surf<-"obs"
obs.1<-cbind(test.auto.data$obs_vol,auto.2$surf)
auto.2$surf<-"prd"
proj.1<-cbind(test.iid.data$mu,auto.2$surf)

data.78<-rbind(obs.1, proj.1)
data.78<-data.table(data.78)
data.78$V1<-as.numeric(data.78$V1)
ggplot(data.78, aes(V1, color = V2)) + 
stat_ecdf(alpha = 0.5, position = 'identity')

```


### Effects

Now that the spatial model has passed the modelling assumptions. Let explore the effects that were modelled. In the spatial calibration model the naively projected volume and the VRI height.

```{r, plot_some_stuff, echo = FALSE}
#res<-residuals(chosen)

chosen<-chosen.a
#saveRDS(chosen, "calb_ymodel.rds")
#saveRDS(calb_data4, "calb_data.rds")

#--Min effect
trajectory.min <-
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= min(proj_height_1),
                   proj_age_1= min(proj_age_1),
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))

new.dist.min <- predictAll(chosen, newdata = trajectory.min)
trajectory.min$mu <- new.dist.min$mu
trajectory.min$sigma <- new.dist.min$sigma
trajectory.min$upper.2 <- with(new.dist.min, qGA(0.95, mu = mu, sigma = sigma))
trajectory.min$lower.2 <- with(new.dist.min, qGA(0.05, mu = mu, sigma = sigma))
trajectory.min$upper.1 <- with(new.dist.min, qGA(0.67, mu = mu, sigma = sigma))
trajectory.min$lower.1 <- with(new.dist.min, qGA(0.33, mu = mu, sigma = sigma))

#--Mean effect
trajectory.mean <-
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= mean(proj_height_1),
                   proj_age_1= mean(proj_age_1),
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))
new.dist.mean <- predictAll(chosen, newdata = trajectory.mean)
trajectory.mean$mu <- new.dist.mean$mu
trajectory.mean$sigma <- new.dist.mean$sigma
trajectory.mean$upper.2 <- with(new.dist.mean, qGA(0.95, mu = mu, sigma = sigma))
trajectory.mean$lower.2 <- with(new.dist.mean, qGA(0.05, mu = mu, sigma = sigma))
trajectory.mean$upper.1 <- with(new.dist.mean, qGA(0.67, mu = mu, sigma = sigma))
trajectory.mean$lower.1 <- with(new.dist.mean, qGA(0.33, mu = mu, sigma = sigma))

#Max effect
trajectory.max <-
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= max(proj_height_1),
                   proj_age_1= max(proj_age_1),
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))

new.dist.max <- predictAll(chosen, newdata = trajectory.max)

trajectory.max$mu <- new.dist.max$mu
trajectory.max$sigma <- new.dist.max$sigma
trajectory.max$upper.2 <- with(new.dist.max, qGA(0.95, mu = mu, sigma = sigma))
trajectory.max$lower.2 <- with(new.dist.max, qGA(0.05, mu = mu, sigma = sigma))
trajectory.max$upper.1 <- with(new.dist.max, qGA(0.67, mu = mu, sigma = sigma))
trajectory.max$lower.1 <- with(new.dist.max, qGA(0.33, mu = mu, sigma = sigma))


p.min <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield ", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield ", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.min, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.min) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.min) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.min) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.min) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

p.max <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield ", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield ", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.max, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.max) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.max) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.max) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.max) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

p.mean <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield (", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield (", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.mean, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.mean) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.mean) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.mean) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.mean) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

plot_grid(p.min, p.mean,p.max, labels = c("min(ht)", "mean(ht)", "max(ht)"))
```


# Conclusions

* Larger projected volumes inherently have more uncertainty. 

* VRI projected height is negatively related to uncertainty.

## Exercises

To demonstrate possible spatial effects of clustering use the calibration model in [forestryCLUS](https://github.com/bcgov/clus/tree/master/R/SpaDES-modules/forestryCLUS). 

# References

Robinson, A.P., McLarin, M. and Moss, I., 2016. A simple way to incorporate uncertainty and risk into forest harvest scheduling. Forest Ecology and Management, 359, pp.11-18.
