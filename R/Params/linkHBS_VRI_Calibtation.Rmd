---
title: "Estimating uncertainty in forest inventory projections"
output: 
  html_document: 
    keep_md: yes
---

<!--
Copyright 2018 Province of British Columbia
 
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
 
http://www.apache.org/licenses/LICENSE-2.0
 
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.-->


# Introduction

<<<<<<< HEAD
Accurate estimates of timber volume projected through time and across landscapes are needed to make effective strategic forest management decisions. These estimates, and estimates on the state of many other ecological processes of interest to decision makers (e.g., carbon capture, wildlife habitat) rely on accurate projections of forest structure and timber volume from models. A common concern of decision makers is the level of uncertainty behind model projections that occur both over time and across large landscapes (i.e., stand yield model + forest inventory).  For example, the Chief Forester of British Columbia (BC) is a statutory decision maker responsible for establishing the allowable annual cut (AAC) in timber supply units across the province. This desicision relies heavily on information provided by forest inventory data and stand yield models. Not accounting for uncertainty in AAC decisions can potentially result in decreased confidence in AAC targets.   

There are many sources of uncertainty in timber volume projections and these sources may propagate through time in complex ways. Recently, Robinson et al. (2016) recommended a simple way to measure this uncertainty. Their approach was to calibrate model projections against observed timber volumes measured from scaling and cruising operations following harvesting. In particular, the value of the Robinson et al. (2016) approach is two fold: i.) simulated future timber volumes can be adjusted using this calibration, and ii.) uncertainty of timber volume projections can be estimated by calculating error statistics. Robinson et al. (2016) modelled not only the mean response of the relationship between the projected volume and the observed scale volume but also the variance (sigma). These two statistics specify the distributional parameters (mean and scale) of the prediction uncertainty. Using these distributional parameters, the distribution of possible yields for an individual harvest unit can be reconstructed and sampled. In the case of AAC targets, the harvest volume distributions for each harvest unit can then be summed to arrive at a distribution for the total volume harvested. This provides an estimate of the variance (i.e., prediction interval) around the total volume estimate or the AAC. The distributional parameters can also be conditional on stand characteristics which would help explain how uncertainty in yield projection can change between harvest units. Understanding which stand characteristics provide greater or lesser uncertainty in yields can be invaluable for strategic decision makers in priotizing where and when to harvest to minimize uncertainty.    

An important assumption of Robinson et al.'s (2016) approach is that the individual harvest units need to be independant of one another, so that error estimates are not biased when summing the error of individual distributions. While this assumption was not formally tested by the authors, they highlighted that if individual harvest units were positively correlated then they would jointly contribute more to the uncertainty than their summation would suggest, and conversely contribute less to uncertainty if they were negatively correlated. In short, the spatial correlation of volume esitmates from timber units needs to be accounted for to accurately represent uncertainty in estimates across large landscapes.

Robinson et al.'s (2016) approach has not been applied in the province of BC because of challenges with spatially tracking observed scaled volumes. Scaled volume is tracked by the harvest billing system (HBS) which focuses on measuring harvest volumes for accounting of provincal revenue. This tracking is accomplished at a spatial unit known as a timber mark. The timber mark is a unique identifer stamped or marked on the end of each harvested log to associate the log with the specific authority to harvest and move timber. Timber marks are typically assigned to multiple harvested units that may be not be spatially contiguous nor contain a single harvest date. Thus, linking HBS and forest inventory data has been difficult due to the complicated spatial and temporal boundaries of timber marks. In particular, estimating the net area harvested (i.e., netting out roads, wildlife tree patches, etc.) and its spatial boundary can be difficult to determine for historical cutblocks.

Here I develop a calibration model for BC that can be used to provide valuable information about timber projection uncertainty. First, I develop an approach to link timber mark boundaries from the HBS to harvested forest stands, I then build on Robinson et al.'s (2016) approach by developing a calibration model that accounts for spatial correlation of timber volume estimates between harvested forest stands. I then further build on this approach by relating uncertainty estimates to stand characetirics, and highlight what stand types have greater uncertainty in their timber volume yields. The calibration model developed here will support adaptive forest management in BC, by identifying factors leading to greater or lesser uncertainty around strategic decisions. For example, such a model may inform a more robust AAC decision, by helping the Cheif Forster understand situations where uncertainty in yields could lead to over or under estimates in AAC. The proposed calibration model will also be used in the caribou and landuse simulator (CLUS) model to provide an estimate of volume yield uncertainty in the quantification of impacts from caribou conservation activities on harvest flows. 
=======
Estimates of timber volume projected through time and across landscapes are need to assess the sustainability of forestry decisions. The economic value of timber and many ecological processes of interest to decision makers (e.g., biomass, carbon, wildlife habitat) are all driven by projections of forest structure which are often characterized by timber volume. A common concern by forestry decision makers is the level of uncertainty behind these projections across large landscapes. 

There are many sources of uncertainty in timber volume projections and these sources propagate through time in complex ways which may provide barriers for confidentaly making forest management decisions. Past research has highlighted how sources of uncertainty related to errors in forest attributes reported in the forest inventory would propogate with errors in growth and yield models and thus, impact harvest scheduling. Recently, Robinson et al. (2016) recommended a simple way to incorpate this uncertainty into forestry decision making. Their approach relied on observed timber volumes measured from scaling and cruising operations following harvesting. In particular, the value of the Robinson et al. (2016) approach is two fold: i.) a calibration of the future timber volumes for use when simulating the forestry decisions and ii.) the ability to simulate the uncertainty of timber volume projection and thus calculate error statistics useful to decision makers. Robinson et al. (2016) modelled the distributional parameters of a prediction by specifying a seperate model for the mean and scale. These parameters were then used to reconstruct the response distribution for an individual harvest unit which can be summed to arrive at a distribution for the total volume harvested. Thereby allowing the construction of a prediction interval on the total volume harvested.

An important assumption of this approach is that the individual harvest units need to be independant of one another, in order to be able to sum the individual distributions. While this assumption was not formally tested, the authors highlight that if the individual harvest units were positively correlated then they would jointly contribute more to the uncertainty than their summation would suggest and conversely contribute less to uncertainty, if they were negatively correlated. This is an important point that could be leverage to develop harvest schedules that result in reduced uncertainty.

Currently, this approach has not been applied in the province of BC given issues with spatially tracking observed scaled volumes. The tracking of observed scaled volume is accomplished via the harvest billing system (HBS) which focuses on harvest volumes, needed for provincal revenue accounting, at a spatial unit known as a timber mark. The timber mark is a unique identifer stamped or marked on the end of each log to associate the log with the specific authority to harvest and move timber. Timber marks are assigned to harvested units that may be not be spatially contiguous nor contain a single harvest date. Thus, linking HBS and forest inventory data has been difficult due to issues with temporal and spatial accuracy in estimating the spatial boundaries of the timber marks. In particular, estimating the net area harvested (i.e., netting out roads, wildlife tree patches, etc) and its spatial boundary can be difficult to determine for historical cutblocks.

In the following sections, the spatial (timber mark boundaries) and temporal (harvest date, inventory projection) data are manipulated to develop a calibration model that can be used to provide valuable information about timber projection uncertainty. It is believed that this calibration will support adaptive forest management by enhancing the link between strategic decisions and their implementation. The specific objectives were: i) to calibrate projected volume yields from an aggregated growth and yield model (hereafter termed the meta-model) with observed volume yields reported in the harvest billing system and ii) determine how uncertainty in total harvested volumes can be minimized. The proposed calibration model will be used in the caribou and landuse model (CLUS) to provide an estimate of volume yield uncertainty which will help provide some context surrounding the quantification of impacts from caribou conservation activities on harvest flows. 
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

# Methods
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(sf)
library(dplyr)
library(ggplot2)
library(Hmisc)
library(bcmaps)
library(gstat)
library(ape)
library(ade4)
library(plotly)
library(gamlss)
library(cowplot)
library(ncf)
source("C:/Users/KLOCHHEA/clus/R/functions/R_Postgres.R")
```

### Linking Timber Mark Boundaries to Forest Stands

Linking the timber mark boundaries with the VRI (needed to link to a growth and yield model) involved the following: (see [code](https://github.com/bcgov/clus/blob/master/SQL/develop_vri2011table.sql))

<<<<<<< HEAD
1. Subset and estimate the spatial boundaries of the timber marks

The spatial boundaries of the timber mark were estimated using the compilation of two spatial data sets: i) [forest tenure cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/forest-tenure-cutblock-polygons-fta-4-0) (aka. ftn_c_b_pl_polygon) and ii) [consolidated cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/harvested-areas-of-bc-consolidated-cutblocks-) (a.k.a cns_cut_bl_polygon). While the forest tenure polygons provided the spatial boundary of the timber mark, these boundaries include retention areas which can be quite large. Thus, the consolidate cutblock polygons dataset was used to remove these non-harvested areas within a timber mark area.

Using the forest tenure cutblock polygons, all timber marks comprised of harvest units that were spatially contiguous and had disturbance start dates between 2012-01-01 and 2016-12-31 were selected. This ensured timber marks would be roughly at the scale of a harvest unit or cutblock and would temporally link to the forest inventory. Further, timber marks with scaled dead wood were removed since the naive projections do not include standing dead timber. Lastly, timber marks were removed if they did not contain geometries reported by RESULTS (the most accurate form of reporting cutblock information). This ensured a more accurate net spatial description of the timber mark boundary (i.e., retention patches were removed). Despite the accuracy of the RESULTS, there remain a number of issues with identifying non-harvest areas in the consolidated cutblock polygons. Thus, timber marks were removed when the estimated timber mark area was not within $\pm$ 20% of the planned net area.This subset of the forest tenure cutblock polygons was then joined with consolidated cutblock polygons to provide a spatial boundary that exluded non harvest areas. 
=======
Linking the timber mark boundaries with the VRI (needed to link to a growth and yield model) involved the following: (see [code](https://github.com/bcgov/clus/blob/master/SQL/develop_vri2011table.sql))

1. Subset and estimate the spatial boundaries of the timber marks

The spatial boundaries of the timber mark were estimated using the compilation of two spatial data sets: i) [forest tenure cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/forest-tenure-cutblock-polygons-fta-4-0) (aka. ftn_c_b_pl_polygon) and ii) [consolidated cutblock polygons](https://catalogue.data.gov.bc.ca/dataset/harvested-areas-of-bc-consolidated-cutblocks-) (a.k.a cns_cut_bl_polygon). While the forest tenure polygons provided the spatial boundary of the timber mark, these boundaries include retention areas which can be quite large. Thus, the consolidate cutblock polygons dataset was used to remove these non-harvested areas within a timber mark area.

Using the forest tenure cutblock polygons, all timber marks comprised of harvest units that were spatially contiguous and had disturbance start dates between 2012-01-01 and 2016-12-31 were selected. This ensured timber marks would be roughly at the scale of a harvest unit or cutblock and would temporally link to the forest inventory. Further, timber marks with scaled dead wood were removed since the naive projections do not include standing dead timber. Lastly, timber marks were removed if they did not contained geometries reported by RESULTS (the most accurate form of reporting cutblock information). This ensured a more accurate net spatial description of the timber mark boundary (i.e., retention patches were removed). Despite the accuracy of the RESULTS, there remain a number of issues with identifying non-harvest areas in the consolidated cutblock polygons. Thus, timber marks were removed when the estimated timber mark area was not within $\pm$ 20% of the planned net area.This subset of the forest tenure cutblock polygons was then joined with consolidated cutblock polygons to provide a spatial boundary that exluded non harvest areas. 
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

2. Intersect the timber mark spatial boundaries with the VRI

The resulting spatial boundaries that excluded non-harvested areas were then spatialy intersected with the 2011 VRI to provide the necessary forest attribute information require to link to the growth and yield model. Only the dominant layer (layer rank 1) of the forest inventory was used, thereby simplifying the growth and yield proejction. As a result of this intersection, 1264 of the timber marks had a portion of their total area that failed to provide the neccessary forest inventory information (i.e., areas that lack the publically available forest inventory such as Tree Farm Licenses). 

The following is a histogram of the percentage of the total timber mark area missing information:

```{r, timber_mrks, echo = FALSE, fig.cap = 'FIGURE 1. A histogram of the percentage of the total timber mark area that did not match with forested VRI polygons. q25 and q75 are the 25th and 75th quantile, respectively.'}
feats<-getSpatialQuery("SELECT feature_id, shape, yc_grp, bec_zone_code, site_index, crown_closure, pcnt_dead, proj_height_1, proj_age_1 FROM yt_vri2011")
#cnx_hbs<-getSpatialQuery("SELECT * FROM cnx_hbs")
<<<<<<< HEAD

cnx_hbs<-getSpatialQuery("SELECT * FROM cnx_hbs where timber_mrk in (select timber_mrk from (
select count(*) as no_opens, foo.timber_mrk from (select count(*) as no_opening_id, opening_id, timber_mrk from cnx_hbs 
group by opening_id, timber_mrk order by timber_mrk) as foo
group by foo.timber_mrk) as foo2 where no_opens = 1)")

=======

cnx_hbs<-getSpatialQuery("SELECT * FROM cnx_hbs where timber_mrk in (select timber_mrk from (
select count(*) as no_opens, foo.timber_mrk from (select count(*) as no_opening_id, opening_id, timber_mrk from cnx_hbs 
group by opening_id, timber_mrk order by timber_mrk) as foo
group by foo.timber_mrk) as foo2 where no_opens = 1)")

>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
cnx_hbs<-lwgeom::st_make_valid(cnx_hbs)

polys<- st_intersection(cnx_hbs, feats) #spatialy join the timber mark boundaries with the id that links the VRI
polys$area<-st_area(polys)/10000 #calc the area of the individual polygons within a timber mark
polys$time_since_inv<-polys$harvestyr -2011
polys$age_hrv<-polys$proj_age_1 + polys$time_since_inv


#get elevation?
ras.elv<-getTableQuery("SELECT  timber_mrk, (stats).*
FROM (SELECT timber_mrk, ST_SummaryStats(ST_Clip(rast, feat.wkb_geometry)) As stats
      FROM rast.dem
      INNER JOIN  (SELECT timber_mrk, st_union(wkb_geometry) as wkb_geometry 
				   FROM cnx_hbs where timber_mrk in (select timber_mrk from (
select count(*) as no_opens, timber_mrk from (select count(*) as no_opening_id, opening_id, timber_mrk from cnx_hbs 
group by opening_id, timber_mrk order by timber_mrk) as foo
group by timber_mrk) as foo2 where no_opens = 1   ) group by timber_mrk
   ) as feat
    ON ST_Intersects(feat.wkb_geometry,rast) 
 ) As foo")
    
#Create a temp table?
conn<-GetPostgresConn()
dbWriteTable(conn, "polys",polys)
dbDisconnect(conn)

yt_prj<-getTableQuery("SELECT t.feature_id, t.yc_grp, t.bec_zone_code,
  (((k.prj_vol_dwb - y.prj_vol_dwb*1.0)/10)*(t.age_hrv - CAST(t.age_hrv/10 AS INT)*10))+ y.prj_vol_dwb as itvol
  FROM polys t
  LEFT JOIN vdyp_test3 y 
  ON t.yc_grp = y.yc_grp AND CAST(t.age_hrv/10 AS INT)*10 = y.prj_total_age
  LEFT JOIN vdyp_test3 k 
  ON t.yc_grp = k.yc_grp AND round(t.age_hrv/10+0.5)*10 = k.prj_total_age WHERE t.age_hrv > 0
  ;")

#delete the temp table
conn<-GetPostgresConn()
dbExecute(conn, "Drop table polys;")
dbDisconnect(conn)

#join with poly
out.tbl<-merge(polys, yt_prj)
out.tbl$vol<-out.tbl$area*out.tbl$itvol #calc the volumes for each of the individual polygons within a timber mark
#st_write(out.tbl, "test3.shp")

#get the itvols that are NULL == don't match with the meta-model
out.tbl2<-data.table(out.tbl)
units(out.tbl2$area) <- NULL
units(out.tbl2$vol) <- NULL

#43118
out.tbl2<-out.tbl2[!(timber_mrk == 'BY5H37'),] #this timber mark also has a missing portion 

test.1 <-data.table(out.tbl2)
number_obs<-test.1[, .(count = .N, var = sum(area)), by = timber_mrk ]

#figure out the distrubution of area with no matching
noMatchArea<-out.tbl2[is.na(itvol), sum(area), by =timber_mrk]
setnames(noMatchArea, "V1", "area")
totalArea<-out.tbl2[, sum(area), by =timber_mrk]
setnames(totalArea, "V1", "totarea")
percentNoMatch<-merge(noMatchArea, totalArea)
percentNoMatch$pernomatch<-percentNoMatch$area/percentNoMatch$totarea


#plot the distribution
eq <- substitute(italic(q25)== a*","~~italic(q75)== b ,list(a=as.numeric(quantile(percentNoMatch$pernomatch,0.25)), b= as.numeric(quantile(percentNoMatch$pernomatch,0.75))))
  
ggplot(data = data.table(per.Area.Missing=percentNoMatch$pernomatch), aes(x=per.Area.Missing)) +
  geom_histogram(bins =120)+
  geom_vline(xintercept=quantile(percentNoMatch$pernomatch,0.25))+
  geom_vline(xintercept=quantile(percentNoMatch$pernomatch,0.75))+
  geom_text(aes(x = 0.30, y =400 , label = as.character(as.expression(eq)) ), parse = TRUE) + 
  theme_bw()

```

This histogram shows that 75% of timber marks were missing forest inventory attribution for only 1.4% of their total area. After visually checking, two issues arose: i) the spatial boundaries of these timber marks extend into non-forested area as reported by the VRI; and ii) there wasn't enough information to parameterize the growth and yield model (outside the domain of the inputs, e.g., recently disturbed). In the case of the non-forested areas, these could be small spatial errors in the VRI and were thus important sources of uncertainty. However, from a practical view, these relatively small areas would contribute little to the total projected volume estimate. Thus, timber marks with greater than 3 percent of their total area containing inadequate forest inventory information were removed from the analysis.


```{r, sample, echo = FALSE, fig.cap = 'FIGURE 2. The location of timber marks used in the analysis (n = 326).'}
noMatch2<-unique(percentNoMatch[pernomatch > 0.03, timber_mrk])
timbr_mrks<-out.tbl2[!(out.tbl2$timber_mrk %in% noMatch2),]
timbr_mrks[is.na(vol), vol:=0]
test.1 <-data.table(timbr_mrks)
number_obs<-test.1[, .(count = .N, var = sum(area)), by = timber_mrk ]

out.tbl3<-timbr_mrks[, sum(vol), by =timber_mrk]
setnames(out.tbl3, c("V1", "timber_mrk"), c("proj_vol","timber_mark"))

out.tbl3.area<-timbr_mrks[, sum(area), by =timber_mrk]
setnames(out.tbl3.area, c("V1", "timber_mrk"), c("area","timber_mark"))

out.tbl4<-merge(out.tbl3,out.tbl3.area)

hbs_obs<-getTableQuery("SELECT timber_mark, sum(volume_m3) as obs_vol FROM hbs_select_tmbr_mrk  where waste_type = 'Non-Waste' AND coast_interior = 'Interior' group by timber_mark")

dead_hbs_obs<-getTableQuery("SELECT distinct(timber_mark) FROM hbs_select_tmbr_mrk where grade = 'CB Dead'")
hbs_obs<-hbs_obs[!(hbs_obs$timber_mark %in% dead_hbs_obs$timber_mark),]

calb_data<-data.table(merge(hbs_obs, out.tbl4))
units(calb_data$area) <- NULL
units(calb_data$proj_vol) <- NULL

calb_data<-calb_data[proj_vol > 100 , ] # get rid of small blcoks that may not be some sort of alternative cutting system
calb_data<-calb_data[obs_vol > 100 , ] # get rid of small blcoks that may not be some sort of alternative cutting system

#check outliers
#'WAWVFF' ,'WBJKDD' , '88606', 'DE2270', '52/911', 'DE2621', 'EU6615'
#WAWVFF is a small triangle problem with mapping the area its 0.6 ha so very small remove
#WBJKDD is also less than 1 ha remove
#No reason to remove 88606 -- looks ok VRI reporting a site index of 10
#No reason to remove DE2270 -- looks ok VRI reporting a site index of 8
#52/911 area wrong remove
#No reason to remove - DE2621 -- VRI site index 10-12 (low)
# Nor reason to remove EU6615 -- VRI site index 8

calb_data<- calb_data[!(timber_mark %in% c('WAWVFF','WBJKDD', '52/911')),]
calb_data$proj_vol<-as.numeric(calb_data$proj_vol)

#Get the spatial X,Y coordinates
cents<-st_read(paste0(here::here(), "/R/Params/centroid_timber_mkrs2.shp"))
cents2<-st_coordinates(st_sf(cents))
cents2<-data.table(cents2)
cents2$timber_mark<-cents$tmbr_mr
calb_data<-merge(calb_data, cents2, by.x = "timber_mark", by.y ="timber_mark", all.x = TRUE)

#Get the elevation
ras.elv<-data.table(ras.elv)
setnames(ras.elv, "timber_mrk", "timber_mark")

ras.elv2<-ras.elv[, weighted.mean(mean, count), by =timber_mark]
ras.elv2[timber_mark == '90884', V1:= 1372]
calb_data<-merge(calb_data, ras.elv2, by.x ="timber_mark", by.y = "timber_mark", all.x = TRUE )
setnames(calb_data, "V1", "elv")
#map the observations?
bec <-get_layer("bec")
ggplot() + geom_sf(data = bec, aes(fill=ZONE), size =0.1)+ geom_point(data=calb_data, aes(x=X, y=Y), color="red")
```


### Growth and yield meta-model

In BC, forested stands from natural orgins are typically projected through time using [Variable Density Yield projection](https://www2.gov.bc.ca/gov/content/industry/forestry/managing-our-forest-resources/forest-inventory/growth-and-yield-modelling/variable-density-yield-projection-vdyp) (VDYP). VDYP is a stand-level empirical growth and yield model that uses VRI attribution as inputs into its sub-models. Using the 2018 vintage of the VRI, each polygon was projected for 350 years using 10 year time steps. The result of this process was a dataset with over 3.5 million yield curves which took 3 days to complete on a intel xeon, 3.5 ghz processor with 64 GB of RAM. Both the input (VRI information) and outputs (yields over time) were uploaded into a PostgreSQL database for further processing. Using the layer 1 rank information (the dominant layer),  yield curve groups (yc_grp) or anlaysis units were constructed using: BEC zone, site index (2 m interval), height class (5 classes as per the VRI) and crown closure class (5 classes as per the VRI). Each yc_grp was then aggregated by area weighting the respective individual polygon level yield curves. The result was a provincial database of composite yield curves that directly links to the VRI through the layer 1 rank attribution described above.

### HBS volumes vs projected meta-model volumes

Each VRI polygon that intersected the timber mark boundary and contained a projected volume was summed to estimate the total projected timber volume for the timber mark. This projected timber volume represented the naive projection commonly used in forest estate modeling.  

```{r, echo = FALSE, Step6_develop_vri2011, fig.cap = 'FIGURE 3. The relationship between observed (obs_vol) and projected (proj_vol) volumes ($m^3$). The yellow line is a one to one relationship; the blue line is a linear line of best fit; the dashed red lines represent the 95% prediction interval (n= 326).'}

model1 <- lm(obs_vol ~ proj_vol, data=calb_data)
summary(model1)
temp_var <- predict(model1, data =calb_data, interval="prediction")
calb_data2 <- cbind(calb_data, temp_var)

lm_eqn = function(m) {
  l <- list(a = format(as.numeric(coef(m)[1]), digits = 2),
      b = format(as.numeric(coef(m)[2]), digits = 2),
      r2 = format(summary(m)$r.squared, digits = 3))
  eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  as.character(as.expression(eq))                 
}

ggplot(calb_data2, aes(proj_vol, obs_vol)) +
  geom_point() +
  geom_smooth(method='lm', se = TRUE)+
  geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
  geom_line(aes(y=upr), color = "red", linetype = "dashed") +
  geom_text(aes(x = 35000, y = 100000, label = lm_eqn(model1)), parse = TRUE) +
    geom_abline(intercept =0, slope=1, col ="yellow") +
  theme_bw()


```

### Modelling assumptions

The following assumptions of the calibration model need to be tested: i) the response is gamma distributed, ii) the model fits the data and iii) the residuals are independant.

<<<<<<< HEAD
From the histogram below the gamma and log normal distributions appear to be candidates distributions to model the response.
=======
From the histogram below the gamma and log normal distributions appear to be candidates.
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

```{r, dist_proj_vol, fig.cap="FIGURE 4. Histogram of the projected volumes of timber marks between 2012 to 2016 (n=326)"}
#library(fitdistrplus)
#param.1 <- MASS::fitdistr(calb_data4$obs_vol, "normal")
#param.2 <- MASS::fitdistr(calb_data4$obs_vol, "lognormal")
#param.3 <- fitdistr(calb_data4$obs_vol, "gamma", start = list(shape = 1.2, scale = 0.0001), lower = 0.01)

hist(calb_data2$obs_vol, prob = T, ylim = c(0, 0.00007))
curve(dgamma(x, 1.2, 0.0001), add=TRUE, col = 'red')
curve(dnorm(x, 15138, 19838), add=TRUE)
curve(dlnorm(x, 8.79, 1.41), add=TRUE, col = 'blue')
```
<<<<<<< HEAD

# Results

### Calibration model

In Robinson et al. (2016) a gamma model was used to model both the  mean and  variance of the response distribution. Gamma models are advantageous because they are highly flexible in the positive domain and allow the modeling of heteroskedastic variance. Below we try a few gamma models by incorporating forest attributes as predictors of both the mean and variance. The results suggest a gamma model is better fit over a log normal model. Also, the VRI height and elevation are important predictors of the response variance. However, these models assume independance which needs to be tested.
=======

## Calibration model

In Robinson et al. (2016) a gamma model was used to model both the  mean and  variance of the response distribution. Gamma models are advantageous because they are highly flexible in the positive domain and allow the modeling of heteroskedastic variance. Below we try a few gamma models by incorporating forest attributes as predictors of both the mean and variance. The results suggest a gamma model is better fit over a log normal model. Also, the VRI height is a important predictor of the response variance. However, these models assume no spatial correlation which needs to be tested.
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

```{r, calibrate_model,  fig.cap= 'FIGURE 4. Fit statistics of the calibration model (n=326)' }
# get the bec zone that makes up the majority of the timber mark 
pv_timber_mrk<-merge(timbr_mrks, totalArea)
pv_timber_mrk[,wt:=as.numeric(area/totarea)]
pv_timber_mrk2<-pv_timber_mrk[, lapply(.SD, function(x) {wtd.mean (x, wt)}), by =timber_mrk, .SDcols=c("proj_height_1", "site_index", "crown_closure", "proj_age_1", "pcnt_dead")]
setnames(pv_timber_mrk2, "timber_mrk" , "timber_mark" )
calb_data3<-merge(calb_data2, pv_timber_mrk2, by = "timber_mark")
calb_data4<-na.omit(calb_data3)

## Fit and compare some models
test.0 <- gamlss(obs_vol ~ proj_vol,
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.0 <- gamlss(obs_vol ~ proj_vol,
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

test.1 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ 1,
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)
test.1 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ 1,
                 mu.link = "log",
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

test.2 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ proj_vol,
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.3 <- gamlss(obs_vol ~ log(proj_vol),
                 sigma.formula = ~ log(proj_vol),
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.4.ga <- gamlss(obs_vol ~ log(proj_vol),
<<<<<<< HEAD
                 sigma.formula = ~ log(proj_vol) + proj_height_1 + elv,
=======
                 sigma.formula = ~ log(proj_vol) + proj_height_1,
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
                 sigma.link = "log",
                 family = GA(),
                 data = calb_data4)

test.4.ln <- gamlss(obs_vol ~ log(proj_vol),
<<<<<<< HEAD
                 sigma.formula = ~ log(proj_vol) + proj_height_1 + elv,
=======
                 sigma.formula = ~ log(proj_vol) + proj_height_1,
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
                 sigma.link = "log",
                 family = LOGNO(),
                 data = calb_data4)

LR.test(test.4.ln, test.4.ga)
chosen<-test.4.ga
summary(chosen)
plot(chosen)
```

### Testing for autocorrelation

<<<<<<< HEAD
An assumption of the uncertainty model is that the errors are independant. This assumption is required in order to sum across many predictions and achieve an unbiased total. Here I test for autocorrelation in the residuals.
=======
An assumption of the uncertainty model is that the errors are independant - in order to be able to sum across many predictions and achieve the appropriate total. Here I test for autocorrelation in the residuals.
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

```{r, testing_auto_correlation}

ind.1<- predictAll(chosen, newdata = calb_data4)
ind.2<-cbind(calb_data4, ind.1$mu)
ind.2$res<-ind.2$obs_vol - ind.2$V2
<<<<<<< HEAD

#get distances
dists <- as.matrix(dist(cbind(ind.2$X, ind.2$Y)))
dists.inv <- 1/dists 
diag(dists.inv) <- 0
#dists[1:50, 1:50] # check what they look like - units are in metres

#auto.2$res<-auto.2$res+runif(326, 1, 10000)
Moran.I(ind.2$res, dists.inv)
#observed is significantly greater than expected  - positively correlated. Thus, jointly contribute more to the uncertainty then their sum would suggest.

xyspatial=SpatialPoints(cbind(ind.2$X,ind.2$Y))
porspatial=data.frame(ind.2$res)
spatialdata=SpatialPointsDataFrame(xyspatial,porspatial)

vario2 <- variogram(ind.2$res~1, spatialdata, cutoff = 3000)
plot(vario2)

bubble(spatialdata, "ind.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")


```

The results from the Moran's I suggest that the residuals are independant (Moran's I = 0.0231, p = 0.0739). After building a variogram there doesn't seem to be a strong spatial effect.  However, the bubble map shows some spatial clustering -namely in the far east corner and around the kamloops area. Also the p value for Moran's I is providing enough evidence against the null hypothesis to warrant a check to see if the model can be further improved by taking into account any spatial autocorrelation.

### Autocorrelated model

To account for the spatial autocorrelation in the model we construct a mixed effect model with on random component. This model specifies the structure of the covariance matrix using a spatial lag between observations. Thus, the off diagonal elements of the variance-covariance matrix will be estimated using a spatial covariance structure that is parameterized from a model assuming independance. The steps to fit this model follow an estimated generalized least squares approach:

1.) Fit a gamma model for the mu and sigma components using fixed effects that assumes i.i.d. covariance structure.

2.) Specify the pattern of the error covariance structure to a spatial lag that assumes a spherical variogram model. Since there is only one group of dependance this random effect is viewed as 'nuisance' and will only be used to get good estimates for the standard errors of the coefficients of the fixed component of the model and the mean of y given values for the x's. Note this is different from predicting a spatially random variable where the random effect is of interest - however, this could not accomplished given the samples used in this study are not well spaced.

3.) Find estimates for the parameters of the covariance matrix. REML is used since the fixed effects part is already estimated and reduces the search to only the covariance parameters.

4.) Re-fit the fixed effects gamma model using the newly parameterized covariance matrix. 

5.) Iterate through 1-4 until convergence criterion has been met

The result of this model is a Moran's I of 0.0229, p = 0.0754, which suggests some of remaining spatial autocorrelation is being accounted. The range of the spherical variogram is 2718 m, with a nugget effect of 0.34. The AIC of this spatial model is smaller (6159.75 versus 6166.3) than the model assuming independance. Thus, it appears the spatial model may be the prefered model.  

```{r, autocorr}
calb_data4$dummy<-1
chosen.a <- gamlss(obs_vol ~log(proj_vol) + re(random=~1|dummy, correlation = corSpher(2500, form = ~ X + Y, nugget = T), opt="optim", method = "REML"),
                   sigma.formula = ~ log(proj_vol) + proj_height_1 + elv ,
                   sigma.link = "log",
                   family = GA(), data = calb_data4, control = gamlss.control(c.crit = 0.005), method=CG())
summary(chosen.a)


plot(chosen.a)

auto.1<- predictAll(chosen.a, newdata = calb_data4)
auto.2<-cbind(calb_data4, auto.1$mu)
auto.2$res<-auto.2$obs_vol - auto.2$V2

=======

#get distances
dists <- as.matrix(dist(cbind(ind.2$X, ind.2$Y)))
dists.inv <- 1/dists 
diag(dists.inv) <- 0
#dists[1:50, 1:50] # check what they look like - units are in metres

#auto.2$res<-auto.2$res+runif(326, 1, 10000)
Moran.I(ind.2$res, dists.inv)
#observed is significantly greater than expected  - positively correlated. Thus, jointly contribute more to the uncertainty then their sum would suggest.

xyspatial=SpatialPoints(cbind(ind.2$X,ind.2$Y))
porspatial=data.frame(ind.2$res)
spatialdata=SpatialPointsDataFrame(xyspatial,porspatial)

vario2 <- variogram(ind.2$res~1, spatialdata, cutoff = 3000)
plot(vario2)
bubble(spatialdata, "ind.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")


```

The results from the Moran's I suggest that the residuals are independant (Moran's I = 0.0239, p = 0.06556). After building a variogram there doesn't seem to be a strong spatial effect.  However, the bubble map shows some spatial clustering -namely in the far east corner and around the kamloops area. Also the p value for Moran's I is providing enough evidence against the null hypothesis to warrant a check to see if the model can be further improved by taking into account any spatial autocorrelation.

### Autocorrelated model

To account for the spatial autocorrelation in the model we construct a random effect with mean zero and covariance structure modelled using a spatial lag between observations. Thus, the off diagonal elements of the variance-covariance matrix will be estimated using a spatial covariance structure that is parameterized from a model assuming independance. The steps to fit this model included:

1.) Fit a gamma model for the mu and sigma.

2.) Use the residuals from this model to construct a spatially varying random variable that is normally distributed with mean 0 and covariance

3.) Specifiy the structure of this covariance matrix to follow a spatial lag that uses a spherical model of the X and Y locations

4.) Re-fit the gamma model with the spatially varying random variable

5.) Iterate through 1-4 until convergence criterion has been met

The result of this model is a Moran's I of 0.0235, p = 0.69, the bubble map shows less clustering, and the AIC of this spatial model is less (6162 versus 6170) than the model assuming independance. Thus, it appears the spatial model may be the prefered model. Lets see how well the predictions look. 

```{r, autocorr}
calb_data4$dummy<-1
chosen.a <- gamlss(obs_vol ~log(proj_vol) + re(random=~1|dummy, correlation = corSpher(2500, form = ~ X + Y, nugget = T), opt="optim", method = "REML"),
                   sigma.formula = ~ log(proj_vol) + proj_height_1 ,
                   sigma.link = "log",
                   family = GA(), data = calb_data4, control = gamlss.control(c.crit = 0.005), method=CG())

plot(chosen.a)

auto.1<- predictAll(chosen.a, newdata = calb_data4)
auto.2<-cbind(calb_data4, auto.1$mu)
auto.2$res<-auto.2$obs_vol - auto.2$V2

>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
Moran.I(auto.2$res, dists.inv)
#There is more evidence for the null hypothesis

xyspatial.auto=SpatialPoints(cbind(auto.2$X,auto.2$Y))
porspatial.auto=data.frame(auto.2$res)
spatialdata.auto=SpatialPointsDataFrame(xyspatial.auto,porspatial.auto)
<<<<<<< HEAD
vario2 <- variogram(auto.2$res~1, spatialdata, cutoff = 5000)
=======
vario2 <- variogram(auto.2$res~1, spatialdata, cutoff = 2500)
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
plot(vario2)

#look at the residuals -- less clustering going on
par(mfrow = c(2,1))
bubble(spatialdata.auto, "auto.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")
bubble(spatialdata, "ind.2.res", col = c("blue", "orange"), main = "Residuals", xlab = "X-coordinates", 
    ylab = "Y-coordinates")

<<<<<<< HEAD
#chosen.a$mu.coefSmo[[1]]$coefficients$random
=======
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
#See how the coefficents change. Note the significance on the intercept. Intercept significance is different.
chosen.a$mu.coefSmo
summary(chosen.a)
summary(chosen)

#Note the AIC is much smaller than the model assuming independance
AIC(chosen.a)
AIC(chosen)

#Much residuals slightly larger with autocorrelated model -- more uncertainty that the indepdnant more would suggest
#sum(auto.2$res**2)-sum(ind.2$res**2)

<<<<<<< HEAD
#Assuming indepdnance model
test.iid.data<-ind.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1", "elv")] 
=======
```

### Comparing the sum of distributional parameters to simulating via monte carlo

In Robinson et al. (2016), each of the response distributions were summed after sampling 10000 samples from each of the response distributions. However, this is computationally slow ( adds ~ 30 seconds for 326 harvest units). Instead of sampling each individual response distribution -- we can sum the means and sigmas because the individual responses are independant. Here is a comparison between mathematically estimating the total response distribution versus simulating it with many samples.

```{r, iid, echo = FALSE}
#Assuming indepdnance model
test.iid.data<-ind.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1")] 
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
test.iid.0<- predictAll(chosen, newdata = test.iid.data) 
test.iid.data$mu<-test.iid.0$mu 
test.iid.data$sigma<-test.iid.0$sigma 
  
#sum(test.iid.data$obs_vol)
#sum(test.iid.data$proj_vol)
#sum(test.iid.data$mu)
#sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))

#est param
#sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))/sum(test.iid.data$mu)
summed<-rGA(20000, mu = sum(test.iid.data$mu), sigma = sqrt(sum((test.iid.data$mu*test.iid.data$sigma)**2))/sum(test.iid.data$mu) )
#quantile(summed, c(0.05, 0.5, 0.95))

#-------
#Autocorrelated model
<<<<<<< HEAD
test.auto.data<-auto.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1", "dummy", "elv")] 
=======
test.auto.data<-auto.2[,c("obs_vol", "proj_vol", "X", "Y", "proj_age_1", "proj_height_1", "dummy")] 
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
test.auto.0<- predictAll(chosen.a, newdata = test.auto.data) 
test.auto.data$mu<-test.auto.0$mu 
test.auto.data$sigma<-test.auto.0$sigma 
  
#sum(test.auto.data$obs_vol)
#sum(test.auto.data$proj_vol)
#sum(test.auto.data$mu)
#sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))

#-------
<<<<<<< HEAD
summed.auto<-rGA(20000, mu = sum(test.auto.data$mu), 
                 sigma = sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))/sum(test.auto.data$mu))
#quantile(summed.auto, c(0.025, 0.05, 0.5, 0.95, 0.975))


```

Now to compare the model assuming independance and the spatial model. Here we see that the spatial model is less biased than the model that assumes independance. The mean of the spatial model is much closer to the observed total.

```{r, spat_mod_compare, echo = FALSE}
=======

#simulate
sim.volume <-
    sapply(1:20000,
           function(x)
             with(test.auto.data,
                  sum(rGA(nrow(test.auto.data), 
                          mu = mu,
                          sigma = sigma))))

#hist(sim.volume)
#quantile(sim.volume, c(0.05, 0.5, 0.95))


#est param via math
#sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))/sum(test.auto.data$mu)
summed.auto<-rGA(20000, mu = sum(test.auto.data$mu), 
                 sigma = sqrt(sum((test.auto.data$mu*test.auto.data$sigma)**2))/sum(test.auto.data$mu))
quantile(summed.auto, c(0.025, 0.05, 0.5, 0.95, 0.975))

# compare the distributiongs for math and sim
data.77<-rbind(data.table(vol = summed.auto, type = "math"), data.table(vol = sim.volume, type = "sim"))
ggplot(data.77, aes(vol, fill = type)) + 
geom_histogram(alpha = 0.5, position = 'identity') 

```

Now lets compare the model assuming independance and the spatial model. Here we see that the spatial model is less biased than the model that assumes independance. The mean of the spatial model is much closer to the observed total.

```{r, spat_mod_compare, echo = FALSE}



>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
data.76<-rbind(data.table(vol = summed, type = "assume_iid"), data.table(vol = summed.auto, type = "spatial"))
ggplot(data.76, aes(vol, fill = type)) + 
geom_histogram(alpha = 0.5, position = 'identity') +
geom_vline(xintercept = sum(test.auto.data$obs_vol), size = 1.5 ) +
geom_text(aes(x=5250000, y=3500, label= "Observed"), hjust = 1)+
geom_vline(xintercept = sum(test.auto.data$proj_vol), size = 1.5, linetype = 'dotted') +
  geom_text(aes(x=6150000, y=3500, label= "Naively Projected"), hjust = 1)+
geom_vline(xintercept = sum(test.auto.data$mu), color = '#619CFF') +
geom_vline(xintercept = sum(test.iid.data$mu), color = '#F8766D') +
<<<<<<< HEAD
  labs(x = "Total Volume (m3)", y = "Count") 

```

## Compare obs vs predicted distributions
=======
  labs(x = "Total Volume (m3)", y = "Count") +
  stat_function(fun = pnorm, n = 101, args = list(mean = 4935127, sd = 358751.2)) 


#mean(test.auto.data$obs_vol)*length(test.auto.data$obs_vol)
#sd.t<-((sd(test.auto.data$obs_vol)/sqrt(length(test.auto.data$obs_vol))))*length(test.auto.data$obs_vol)

#ci<-data.frame(lb=sum(test.auto.data$obs_vol) - (2*sd.t),ub= sum(test.auto.data$obs_vol) + (2*sd.t))


##Very close - essentially the same
```

### Compare obs vs predicted distributions
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

Here we compare the cummulative dsitributions to see any deviations between the predicted and response surfaces. They are very similar - a few deviations most notably at larger predictions of the response.
```{r, cdfs}
#ECDF
auto.2$surf<-"obs"
obs.1<-cbind(test.auto.data$obs_vol,auto.2$surf)
auto.2$surf<-"prd"
proj.1<-cbind(test.iid.data$mu,auto.2$surf)

data.78<-rbind(obs.1, proj.1)
data.78<-data.table(data.78)
data.78$V1<-as.numeric(data.78$V1)
ggplot(data.78, aes(V1, color = V2)) + 
stat_ecdf(alpha = 0.5, position = 'identity')

```


<<<<<<< HEAD
### Fixed effects

Now that the spatial model has passed the modelling assumptions. Let explore the effects that were modelled. In the spatial calibration model the naively projected volume, VRI height and elevation were used.

```{r, plot_some_stuff, echo = FALSE}
#res<-residuals(chosen)

saveRDS(chosen.a, "calb_ymodel.rds")
saveRDS(calb_data4, "calb_data.rds")

=======
### Effects

Now that the spatial model has passed the modelling assumptions. Let explore the effects that were modelled. In the spatial calibration model the naively projected volume and the VRI height.

```{r, plot_some_stuff, echo = FALSE}
#res<-residuals(chosen)
#saveRDS(chosen, "calb_ymodel.rds")
#saveRDS(calb_data4, "calb_data.rds")
chosen<-chosen.a

>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
#--Min effect
trajectory.min <-
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= min(proj_height_1),
<<<<<<< HEAD
                   elv= mean(elv),
=======
                   proj_age_1= min(proj_age_1),
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))

<<<<<<< HEAD
new.dist.min <- predictAll(chosen.a, newdata = trajectory.min)
=======
new.dist.min <- predictAll(chosen, newdata = trajectory.min)
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
trajectory.min$mu <- new.dist.min$mu
trajectory.min$sigma <- new.dist.min$sigma
trajectory.min$upper.2 <- with(new.dist.min, qGA(0.95, mu = mu, sigma = sigma))
trajectory.min$lower.2 <- with(new.dist.min, qGA(0.05, mu = mu, sigma = sigma))
trajectory.min$upper.1 <- with(new.dist.min, qGA(0.67, mu = mu, sigma = sigma))
trajectory.min$lower.1 <- with(new.dist.min, qGA(0.33, mu = mu, sigma = sigma))

<<<<<<< HEAD
#--Min effect elv
trajectory.elv.min <-
=======
#--Mean effect
trajectory.mean <-
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= mean(proj_height_1),
<<<<<<< HEAD
                   elv= min(elv),
=======
                   proj_age_1= mean(proj_age_1),
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))
<<<<<<< HEAD

new.dist.min <- predictAll(chosen.a, newdata = trajectory.elv.min)
trajectory.elv.min$mu <- new.dist.min$mu
trajectory.elv.min$sigma <- new.dist.min$sigma
trajectory.elv.min$upper.2 <- with(new.dist.min, qGA(0.95, mu = mu, sigma = sigma))
trajectory.elv.min$lower.2 <- with(new.dist.min, qGA(0.05, mu = mu, sigma = sigma))
trajectory.elv.min$upper.1 <- with(new.dist.min, qGA(0.67, mu = mu, sigma = sigma))
trajectory.elv.min$lower.1 <- with(new.dist.min, qGA(0.33, mu = mu, sigma = sigma))

#--Mean effect
trajectory.mean <-
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= mean(proj_height_1),
                   elv= mean(elv),
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))
new.dist.mean <- predictAll(chosen.a, newdata = trajectory.mean)
=======
new.dist.mean <- predictAll(chosen, newdata = trajectory.mean)
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
trajectory.mean$mu <- new.dist.mean$mu
trajectory.mean$sigma <- new.dist.mean$sigma
trajectory.mean$upper.2 <- with(new.dist.mean, qGA(0.95, mu = mu, sigma = sigma))
trajectory.mean$lower.2 <- with(new.dist.mean, qGA(0.05, mu = mu, sigma = sigma))
trajectory.mean$upper.1 <- with(new.dist.mean, qGA(0.67, mu = mu, sigma = sigma))
trajectory.mean$lower.1 <- with(new.dist.mean, qGA(0.33, mu = mu, sigma = sigma))

#Max effect
trajectory.max <-
<<<<<<< HEAD
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
                   proj_height_1= max(proj_height_1),
                   elv= mean(elv),
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))

new.dist.max <- predictAll(chosen.a, newdata = trajectory.max)

trajectory.max$mu <- new.dist.max$mu
trajectory.max$sigma <- new.dist.max$sigma
trajectory.max$upper.2 <- with(new.dist.max, qGA(0.95, mu = mu, sigma = sigma))
trajectory.max$lower.2 <- with(new.dist.max, qGA(0.05, mu = mu, sigma = sigma))
trajectory.max$upper.1 <- with(new.dist.max, qGA(0.67, mu = mu, sigma = sigma))
trajectory.max$lower.1 <- with(new.dist.max, qGA(0.33, mu = mu, sigma = sigma))

#Max elv effect
trajectory.elv.max <-
=======
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
  with(calb_data4,
       expand.grid(proj_vol =
                   seq(from = min(proj_vol),
                       to = max(proj_vol),
                       length.out = 100),
<<<<<<< HEAD
                   proj_height_1= mean(proj_height_1),
                   elv= max(elv),
=======
                   proj_height_1= max(proj_height_1),
                   proj_age_1= max(proj_age_1),
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374
                   X = mean(X),
                   Y = mean(Y),
                   dummy = 1
                   ))

<<<<<<< HEAD
new.dist.max <- predictAll(chosen.a, newdata = trajectory.elv.max)

trajectory.elv.max$mu <- new.dist.max$mu
trajectory.elv.max$sigma <- new.dist.max$sigma
trajectory.elv.max$upper.2 <- with(new.dist.max, qGA(0.95, mu = mu, sigma = sigma))
trajectory.elv.max$lower.2 <- with(new.dist.max, qGA(0.05, mu = mu, sigma = sigma))
trajectory.elv.max$upper.1 <- with(new.dist.max, qGA(0.67, mu = mu, sigma = sigma))
trajectory.elv.max$lower.1 <- with(new.dist.max, qGA(0.33, mu = mu, sigma = sigma))
=======
new.dist.max <- predictAll(chosen, newdata = trajectory.max)

trajectory.max$mu <- new.dist.max$mu
trajectory.max$sigma <- new.dist.max$sigma
trajectory.max$upper.2 <- with(new.dist.max, qGA(0.95, mu = mu, sigma = sigma))
trajectory.max$lower.2 <- with(new.dist.max, qGA(0.05, mu = mu, sigma = sigma))
trajectory.max$upper.1 <- with(new.dist.max, qGA(0.67, mu = mu, sigma = sigma))
trajectory.max$lower.1 <- with(new.dist.max, qGA(0.33, mu = mu, sigma = sigma))
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374


p.min <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield ", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield ", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.min, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.min) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.min) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.min) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.min) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

p.max <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield ", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield ", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.max, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.max) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.max) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.max) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.max) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

p.mean <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield (", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield (", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.mean, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.mean) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.mean) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.mean) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.mean) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

<<<<<<< HEAD
p.elv.min <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield (", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield (", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.elv.min, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.elv.min) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.elv.min) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.elv.min) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.elv.min) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

p.elv.max <-
  ggplot(calb_data4, aes(x = proj_vol, y = obs_vol) ) +
  geom_point(alpha=0.4) +
  #facet_wrap(~ ForestQualityClass) +
  xlab(expression(paste("Projected Volume Yield (", m^3, ")"))) +
  ylab(expression(paste("Observed Volume Yield (", m^3, ")"))) +
  geom_line(aes(y = mu, x = proj_vol), color = 'blue', data = trajectory.elv.max, lwd = 1.75) +
  geom_line(aes(y = lower.2, x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.elv.max) +
  geom_line(aes(y = upper.2 , x =  proj_vol), linetype = "dashed", color = 'red', data = trajectory.elv.max) +
  geom_line(aes(y = lower.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.elv.max) +
  geom_line(aes(y = upper.1 , x =  proj_vol), linetype = "dotted", color = 'blue', data = trajectory.elv.max) +
  geom_abline(intercept =0, slope=1, col ="yellow")+
  ylim(0,200000)

plot_grid(p.min, p.mean, p.max, p.elv.min, p.elv.max, labels = c("min(ht)", "mean(all)", "max(ht)", "min(elv)", "max(elv)"))
```


## Results Summary

* Larger projected volumes inherently have more uncertainty. 

* VRI projected height is negatively related to uncertainty.

* Elevation is negatively related to uncertainty.

# Discussion

First, the results from this modelling exercise suggest that the spatial model that accounts for the correlation between residuals is a better model. Using a spherical variogram model allowed individual observations to be weighted more effectively so that the effect of dependance was reduced. This provided a model that produced an estimated total volume that was closer to the observed total volume which is important for determining forestry decisions like AAC targets. 

Next, the effects of the predictor variables in the spatial model showed that the meta-model of vdyp was over predicting total volume in harvet units that had larger volumes. It should be noted that this effect cannot be seperated into errors individually generated by VDYP nor the VRI, but rather their combined effect. The management impications for timber supply models and AAC analysis would be: i) in the strategic design of landscapes, developing more aggregated or larger harvest blocks is often of interest to reduce the fragmentation of the landscape - however the realized volume from these larger blocks was shown to be considerably less than one would expect from a naive projection using the VRI and the meta-model of vdyp, ii) further there is greater uncertainty in the AAC if the harvest units were much larger in size, iii) the uncertainty in AAC targets can also be greater for harvest units that are lower in elevation and harvest units with forest structure that is smaller in terms of tree height. 

I would speculate:

* Larger harvest units may translate into a larger volume harvested and thus greater uncertainty. These larger units could also mean a greater number of different stands would be incorporated into the harvesting profile. Increasing the number of stands in a harvest profile may increase the probability of including polygons within the VRI that are erroneous. Conversely, for harvest units that are realtively smaller but still result in large volumes (i.e., highly productive areas) the total volume could be overestimated given the meta-model of vdyp was not taking into account the vertical structure that is likely to form through the development of the stand. Thus, the meta-model may project stands that are highly productive and are perceived to have growing space, but in fact they do not have the growing space because it is being occupied by vertical layers assumed that are assume to not exist. In other words, not accounting for the multi-layered structure of highly productive could lead to greater uncertainty.

* Lower elevation stands are likely to support mixed species which the meta model of vdyp and vdyp have a hard time predicting. Mixed species (and vertical layers as mentioned above) are porjected as individual species (orlayers) and then aggregated, thus removing the true intimate nature of how these species would grow.

* Harvest units that have smaller tree heights are uncertain because the meta model of vdyp typically uses plots of older mature stands near rotation age to calibrate its algorithums. These smaller height stands may not be well represented?

# Exercises

To demonstrate possible spatial effects of clustering use the calibration model in [forestryCLUS](https://github.com/bcgov/clus/tree/master/R/SpaDES-modules/forestryCLUS). 

# APPENDIX: Comparing the sum of distributional parameters to simulating via monte carlo

In Robinson et al. (2016), each of the response distributions were summed after sampling 10000 samples from each of the response distributions. However, this is computationally slow ( adds ~ 30 seconds for 326 harvest units). Instead of sampling each individual response distribution -- we can sum the means and sigmas because the individual responses are independant. Here is a comparison between mathematically estimating the total response distribution versus simulating it with many samples. By mathematically estimating the total response distribution, there is a large cost savings (~25 seconds) which when amplified over a simulation model with a 100 year time horizon and annual time step would result in ~ 42 minutes saved per simulation run. 

```{r, iid, echo = FALSE}
#simulate
sim.volume <-
    sapply(1:20000,
           function(x)
             with(test.auto.data,
                  sum(rGA(nrow(test.auto.data), 
                          mu = mu,
                          sigma = sigma))))

# compare the distributiongs for math and sim
data.77<-rbind(data.table(vol = summed.auto, type = "math"), data.table(vol = sim.volume, type = "sim"))
ggplot(data.77, aes(vol, fill = type)) + 
geom_histogram(alpha = 0.5, position = 'identity') 

```
=======
plot_grid(p.min, p.mean,p.max, labels = c("min(ht)", "mean(ht)", "max(ht)"))
```


# Conclusions

* Larger projected volumes inherently have more uncertainty. 

* VRI projected height is negatively related to uncertainty.

## Exercises

To demonstrate possible spatial effects of clustering use the calibration model in [forestryCLUS](https://github.com/bcgov/clus/tree/master/R/SpaDES-modules/forestryCLUS). 
>>>>>>> 6a205ab3d6b4f768bd550445e95a1b6bda9a5374

# References

Robinson, A.P., McLarin, M. and Moss, I., 2016. A simple way to incorporate uncertainty and risk into forest harvest scheduling. Forest Ecology and Management, 359, pp.11-18.
