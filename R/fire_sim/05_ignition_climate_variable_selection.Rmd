---
title: "05_ignition_climate_variable_selection"
author: "Elizabeth Kleynhans"
date: "08/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Copyright 2020 Province of British Columbia
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and limitations under the License.

#=================================
#  Script Name: 05_ignition_climate_variable_selection.R
#  Script Version: 1.0
#  Script Purpose: Run logistic regression models to determine the top candidate climate variable to use in the final analysis of fire ignitions.
#  Script Author: Elizabeth Kleynhans, Ecological Modeling Specialist, Forest Analysis and Inventory Branch, B.C. Ministry of Forests, Lands, and Natural Resource Operations.
#  Script Contributor: Cora Skaien, Ecological Modeling Specialist, Forest Analysis and Inventory Branch, B.C. Ministry of Forests, Lands, and Natural Resource Operations.
#=================================

#Overview
The purpose of this script is to prepare additional climate data, sub-sample points where no fires occurred (for a better ratio of fire to no fire), and perform model selection to determine the climate variable that best predicts fire occurence for each BEC zone.

```{r}

library(sf)
library(tidyverse)
library(ggplot2)
library (ggcorrplot)
library (RPostgreSQL)
library (rpostgis)
library (dplyr)
library (lme4)
library (arm)
library(ggpubr)
library(mgcv)
library(nlme)
library(purrr)
library(tidyr)
library(caret)
library(pROC)
library(keyring)
library(ggcorrplot) 

source(here::here("R/functions/R_Postgres.R"))
```

Now we must bring in the data that we created at the end of 04_VRI_data_prep. We need only the lightning + NA and the person + NA data sets here. We also need the fire ignition data from the end of our first file (01_fire_ignition_data_prep)

```{r}
#bring in lightning caused fire data
connKyle <- dbConnect(drv = RPostgreSQL::PostgreSQL(), 
                      host = key_get('dbhost', keyring = 'postgreSQL'),
                      user = key_get('dbuser', keyring = 'postgreSQL'),
                      dbname = key_get('dbname', keyring = 'postgreSQL'),
                      password = key_get('dbpass', keyring = 'postgreSQL'),
                      port = "5432")
fire_veg_data_lightning_NA_5x <- sf::st_read  (dsn = connKyle, # connKyle
                               query = "SELECT * FROM public.fire_veg_data_lightning_NA_5x_NoW")
dbDisconnect (connKyle)

#Bring in person-caused fire data
connKyle <- dbConnect(drv = RPostgreSQL::PostgreSQL(), 
                      host = key_get('dbhost', keyring = 'postgreSQL'),
                      user = key_get('dbuser', keyring = 'postgreSQL'),
                      dbname = key_get('dbname', keyring = 'postgreSQL'),
                      password = key_get('dbpass', keyring = 'postgreSQL'),
                      port = "5432")
fire_veg_data_person_NA_5x <- sf::st_read  (dsn = connKyle, # connKyle
                               query = "SELECT * FROM public.fire_veg_data_person_NA_5x_NoW")
dbDisconnect (connKyle)

#Bring in fire ignition data from end of 01 file  
conn <- dbConnect(drv = RPostgreSQL::PostgreSQL(), 
                      host = key_get('dbhost', keyring = 'postgreSQL'),
                      user = key_get('dbuser', keyring = 'postgreSQL'),
                      dbname = key_get('dbname', keyring = 'postgreSQL'),
                      password = key_get('dbpass', keyring = 'postgreSQL'),
                      port = "5432")

fire_ignitions <- sf::st_read  (dsn = conn, # connKyle
                               query = "SELECT * FROM public.bc_fire_ignition")
dbDisconnect (conn) # connKyle

```

Inspect the fire ignition data.

```{r}

fire_ignitions1<-st_set_geometry(fire_ignitions,NULL) # remove geometry column for dataset

# look at histogram of when fires were ignited per year
fire_ignitions1$month<- substring(fire_ignitions1$ign_date, 5, 6)

# Histogram of lightning caused fires
fire_ignitions1_new<- fire_ignitions1 %>%
  filter(fire_year >=2002) %>%
  filter(fire_cause=="Lightning")
fire_ignitions1_new$month<- as.numeric(fire_ignitions1_new$month)
hist(fire_ignitions1_new$month, xlab="Month", main="Histogram of lightning caused fires") # most lightning fires appear to occur between May - Sept, with a peak in July and August!

#Histogram of human caused fires
fire_ignitions1_person<- fire_ignitions1 %>%
  filter(fire_year >=2002) %>%
  filter(fire_cause=="Person")
fire_ignitions1_person$month<- as.numeric(fire_ignitions1_person$month)
hist(fire_ignitions1_person$month, xlab="Month", main="Histogram of person caused fires") # Person caused fires occur throughout the year but also peak in July and August!

table(fire_ignitions1_new$fire_year, fire_ignitions1_new$fire_cause)
table(fire_ignitions1_person$fire_year, fire_ignitions1_person$fire_cause)
```

We must subsample the data since the sample sizes for the zeros are too large. At least two different papers used 1.5 times the number of presence points as absences. See:
   - Chang et al (2013) Predicting fire occurrence patterns with logistic regression in Heilongjiang Province, China. Landscape Ecology 28, 1989-2004 and
   - Catry et al. (2009) Modeling and mapping wildfire ignition risk in Portugal. International Journal of Wildland Fire 18, 921-931.

# Steps for subsampling:
First, get sample sizes per habitat type and year for the 1's. 
Then sample 2x or some amount more than that value in each of those vegetation, year categories. 
See https://jennybc.github.io/purrr-tutorial/ls12_different-sized-samples.html for an idea on how to do this.

This will be done separately for lightning and person caused fires. We will first prepare the tables for both lightning and person caused fires, and then we will perform for each separately.

```{r}
#Set geometry to null for processing below
fire_veg_data_lightning_NA_5x_<-st_set_geometry(fire_veg_data_lightning_NA_5x, NULL)
fire_veg_data_person_NA_5x_<-st_set_geometry(fire_veg_data_person_NA_5x, NULL)
```


The step preparing the tables can take a very long time for the NAs since there are a few hundred thousand. Once you prepare these tables, you do not need to prepare them again as long as they are still saved in your R environment.

```{r}
##Lightning caused - preparing the tables
pre<- fire_veg_data_lightning_NA_5x_ %>%
  filter(fire==1) %>%
  dplyr::select(fire_yr, fire, zone, bclcs_level_2) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%
  summarize(fire_n=n())
head(pre)

##Get NA points
pre_checkpre<- fire_veg_data_lightning_NA_5x_ %>%
  filter(fire==0) %>%
  dplyr::select(fire_yr, fire, zone, bclcs_level_2) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%
  summarize(abs_n=n())
head(pre_checkpre)
```

This next chunk also takes quite a bit of time.

```{r}
check<-left_join(pre, pre_checkpre)
check %>% print(n=100) # hmm there are some NA's in the 0 column. I should probably correct that.
```


```{r}

abs_match <- fire_veg_data_lightning_NA_5x_ %>%
  filter(fire == 0) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%   # prep for work by yr and veg type
  nest() %>%              # --> one row per yr and vegtype
  ungroup()

df<-left_join(check, abs_match) # make sure there are not veg year combinations that are not also in the fire_pres==1 file


# there are several year, zone, subzone combinations with no data in the tibble.  This code below removes the Null values. I should increase my sample of fire absences so that I don't have any combinations with zero data or sample it in a different way. TO DO!
df2 <- df %>% 
  filter(lengths(data)>0)
```

Note: each time you subsample with the code below, you will end up with different random points where fires did not occur. As a result, the results of the subsequent model selection may differ. You may wish to create different random points each a few times to ensure that the same climate variables come out as the top ones for each BEC zone, especially for those with small sample sizes for where fires occurred.

```{r}
# here I sample from the tibble the number of data points I want for the absences
# I should probably have replace = false but there are a few rows where there are more fire ignitions in that subzone than randomly sampled locations which is causing issues with this code. For now I'll leave it like this.
  sampled_df<- df2 %>% 
    mutate(samp = map2(data, ceiling(fire_n*2), sample_n, replace=TRUE)) %>%
    dplyr::select(-data) %>%
    unnest(samp) %>%
    dplyr::select(fire_yr, zone, subzone, bclcs_level_2, feature_id:vegtype)
 
# joining my subsampled absence data back to the fire ignition presence data
pre1<- fire_veg_data_lightning_NA_5x_ %>%
  filter(fire==1)
dim(sampled_df) # 22715 rows
dim(pre1) #  15228 rows

dat_lightning<- rbind(pre1, as.data.frame(sampled_df))
dim(dat_lightning) # 37673 rows good this worked I think; Cora July 5 has 45656 rows. This is fewer than the >180,000 rows of the data at the end of file 01

head(dat_lightning)
str(dat_lightning)
table(dat_lightning$fire_cs)
```

If all looks good from above, then save data.

```{r}
#Write file
st_write(dat_lightning, dsn = "D:\\Fire\\fire_data\\raw_data\\Fire_climate_DEM_VRI_shape_files\\dat_lightning_for_analysis.shp", delete_layer=TRUE)

write.csv(dat_lightning, "D:\\Fire\\fire_data\\raw_data\\Fire_climate_DEM_VRI_shape_files\\dat_lightning_for_analysis.csv")

```

Now we will repeat the process for person-caused fires.

```{r}
##Person caused fires: table preparations
pre2<- fire_veg_data_person_NA_5x_ %>%
  filter(fire==1) %>%
  dplyr::select(fire_yr, fire, zone, bclcs_level_2) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%
  summarize(fire_n=n())

pre_checkpre<- fire_veg_data_person_NA_5x_ %>%
  filter(fire==0) %>%
  dplyr::select(fire_yr, fire, zone, bclcs_level_2) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%
  summarize(abs_n=n())

check2<-left_join(pre2, pre_checkpre)
check2 %>% print(n=100) # hmm there are some NA's in the 0 column. I should probably correct that.

abs_match2 <- fire_veg_data_person_NA_5x_ %>%
  filter(fire == 0) %>%
  group_by(fire_yr, zone, bclcs_level_2) %>%   # prep for work by yr and veg type
  nest() %>%              # --> one row per yr and vegtype
  ungroup()

dfp<-left_join(check2, abs_match2) # make sure there are not veg year combinations that are not also in the fire_pres==1 file


# there are several year, zone, subzone combinations with no data in the tibble.  This code below removes the Null values. I should increase my sample of fire absences so that I don't have any combinations with zero data or sample it in a different way. TO DO!
df2 <- dfp %>% 
  filter(lengths(data)>0)
```

Now subsample from the created tables. Once again, if the above tables remain in your R Environment, you can subsample a few times to run the climate variable selection analyses.

```{r}
# here I sample from the tibble the number of data points I want for the absences
# I should probably have replace = false but there are a few rows where there are more fire ignitions in that subzone than randomly sampled locations which is causing issues with this code. For now I'll leave it like this.
  sampled_df<- df2 %>% 
    mutate(samp = map2(data, ceiling(fire_n*2), sample_n, replace=TRUE)) %>%
    dplyr::select(-data) %>%
    unnest(samp) %>%
    dplyr::select(fire_yr, zone, subzone, bclcs_level_2, feature_id:vegtype)
 
# joining my subsampled absence data back to the fire ignition presence data
pre2<- fire_veg_data_person_NA_5x_ %>%
  filter(fire==1)
dim(sampled_df) #  rows
dim(pre2) #   rows

dat_person<- rbind(pre2, as.data.frame(sampled_df))
dim(dat_person) # 

head(dat_person)
str(dat_person)
table(dat_person$fire_cs)
```

If data looks good, save file.

```{r}
#Write file
#st_write(dat_person, dsn = "D:\\Fire\\fire_data\\raw_data\\Fire_climate_DEM_VRI_shape_files\\dat_person_for_analysis.shp", delete_layer=TRUE) 

write.csv(dat_person,"D:\\Fire\\fire_data\\raw_data\\Fire_climate_DEM_VRI_shape_files\\dat_person_for_analysis.csv")

```


Now that the data is prepped, we can move on to the analysis for selecting the best climatic variable(s) for each BEC zone. The loop below will be run first for lightning caused fires, and then for person-caused fires.

To select the best single fire weather covariate we first conducted exploratory graphical analyses of the correlations between fire frequency and various fire weather variables. Then we fit generalized linear models for each fire weather variable (Eq. 1) using a binomial error structure with logarithmic link. Candidate variables were monthly average temperature, monthly maximum temperature, monthly precipitations and the six mean drought codes (MDCâ€™s). We also added various two, three or fourth-month means of these values (e.g. for May, June, July and August) to test for seasonal effects (e.g. spring vs. summer).

Before we begin the analyses, we will look at some correlations.


```{r}
#### looking at correlations between variables####

####1. Lightning ########

# correlation between max T and MDC. Across all the data MDC and Tmax are correlated with values ranging between 0.78 and 0.15
dist.cut.corr <- dat_lightning [c (19:23, 34:38)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between maximum temperature and MDC")

# Correlation between total precipitation and MDC. This is also pretty correlated with values between -0.75 and -0.18
dist.cut.corr <- dat_lightning [c (29:33, 34:38)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and MDC")

# Correlation between Tmax and total precipitation. The correlation is very low, as would be expected (-0.02 to -0.57) 
dist.cut.corr <- dat_lightning [c (19:23, 29:33)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and Tmax")


# Correlation between Tave and total precipitation. The correlation is very low, as would be expected (-0.03 to -0.43) 
dist.cut.corr <- dat_lightning [c (24:28, 29:33)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and Tave")
```


```{r}
######## Person Caused Fires #############


# correlation between max T and MDC. Across all the data MDC and Tmax are correlated with values ranging between 0.78 and 0.15
dist.cut.corr <- dat_person [c (19:23, 34:38)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between maximum temperature and MDC")

# Correlation between total precipitation and MDC. This is also pretty correlated with values between -0.75 and -0.18
dist.cut.corr <- dat_person [c (29:33, 34:38)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and MDC")

# Correlation between Tmax and total precipitation. The correlation is very low, as would be expected (-0.02 to -0.57) 
dist.cut.corr <- dat_person [c (19:23, 29:33)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and Tmax")


# Correlation between Tave and total precipitation. The correlation is very low, as would be expected (-0.03 to -0.43) 
dist.cut.corr <- dat_person [c (24:28, 29:33)]
corr <- round (cor (dist.cut.corr), 3)
ggcorrplot (corr, type = "lower", lab = TRUE, tl.cex = 10,  lab_size = 3,
            title = "Correlation between total precipitation and Tave")

```


#################################
# ANALYSIS OF CLIMATE VARIABLES
#################################

Loosely following the methods of Marchal et al. (2017) Ecography (https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01849) Supporting Information Appendix 1 I try to figure out which is the best climate variable or climate variables to include in my model. I run simple models of the form:

     logb(p/1-p) = B0 + B1x1 or logb(p/1-p) = B0 + B1x1 + B2x2

and extract the AIC as a means for comparison. I also calculate the AUC by splitting the data into a training and validation data set. Finally I repeat the analysis calculating the AIC and AUC using training and validation data sets 10 times taking the average of both the AIC and AUC values. These are the values that I spit out into a csv file so that I can examine which climate variable is best for each BEC zone. 

Before we begin, we will create additional climate variables.

```{r}
#Climate variable creation for lightning data

### creating amalgamations of variables to test different combinations of variables.##
dat_lightning$mean_tmax05_tmax06<- (dat_lightning$tmax05+ dat_lightning$tmax06)/2
dat_lightning$mean_tmax06_tmax07<- (dat_lightning$tmax06+ dat_lightning$tmax07)/2
dat_lightning$mean_tmax07_tmax08<- (dat_lightning$tmax07+ dat_lightning$tmax08)/2
dat_lightning$mean_tmax08_tmax09<- (dat_lightning$tmax08+ dat_lightning$tmax09)/2
dat_lightning$mean_tmax05_tmax06_tmax07<- (dat_lightning$tmax05+ dat_lightning$tmax06 + dat_lightning$tmax07)/3
dat_lightning$mean_tmax06_tmax07_tmax08<- (dat_lightning$tmax06+ dat_lightning$tmax07 + dat_lightning$tmax08)/3
dat_lightning$mean_tmax07_tmax08_tmax09<- (dat_lightning$tmax07+ dat_lightning$tmax08 + dat_lightning$tmax09)/3
dat_lightning$mean_tmax05_tmax06_tmax07_tmax08<- (dat_lightning$tmax05 + dat_lightning$tmax06+ dat_lightning$tmax07 + dat_lightning$tmax08)/4
dat_lightning$mean_tmax06_tmax07_tmax08_tmax09<- (dat_lightning$tmax06 + dat_lightning$tmax07+ dat_lightning$tmax08 + dat_lightning$tmax09)/4
dat_lightning$mean_tmax05_tmax06_tmax07_tmax08_tmax09<- (dat_lightning$tmax05 + dat_lightning$tmax06 + dat_lightning$tmax07+ dat_lightning$tmax08 + dat_lightning$tmax09)/5

dat_lightning$mean_tave05_tave06<- (dat_lightning$tave05+ dat_lightning$tave06)/2
dat_lightning$mean_tave06_tave07<- (dat_lightning$tave06+ dat_lightning$tave07)/2
dat_lightning$mean_tave07_tave08<- (dat_lightning$tave07+ dat_lightning$tave08)/2
dat_lightning$mean_tave08_tave09<- (dat_lightning$tave08+ dat_lightning$tave09)/2
dat_lightning$mean_tave05_tave06_tave07<- (dat_lightning$tave05+ dat_lightning$tave06 + dat_lightning$tave07)/3
dat_lightning$mean_tave06_tave07_tave08<- (dat_lightning$tave06+ dat_lightning$tave07 + dat_lightning$tave08)/3
dat_lightning$mean_tave07_tave08_tave09<- (dat_lightning$tave07+ dat_lightning$tave08 + dat_lightning$tave09)/3
dat_lightning$mean_tave05_tave06_tave07_tave08<- (dat_lightning$tave05 + dat_lightning$tave06+ dat_lightning$tave07 + dat_lightning$tave08)/4
dat_lightning$mean_tave06_tave07_tave08_tave09<- (dat_lightning$tave06 + dat_lightning$tave07+ dat_lightning$tave08 + dat_lightning$tave09)/4
dat_lightning$mean_tave05_tave06_tave07_tave08_tave09<- (dat_lightning$tave05 + dat_lightning$tave06 + dat_lightning$tave07+ dat_lightning$tave08 + dat_lightning$tave09)/5


dat_lightning$mean_ppt05_ppt06<- (dat_lightning$ppt05+ dat_lightning$ppt06)/2
dat_lightning$mean_ppt06_ppt07<- (dat_lightning$ppt06+ dat_lightning$ppt07)/2
dat_lightning$mean_ppt07_ppt08<- (dat_lightning$ppt07+ dat_lightning$ppt08)/2
dat_lightning$mean_ppt08_ppt09<- (dat_lightning$ppt08+ dat_lightning$ppt09)/2
dat_lightning$mean_ppt05_ppt06_ppt07<- (dat_lightning$ppt05+ dat_lightning$ppt06 + dat_lightning$ppt07)/3
dat_lightning$mean_ppt06_ppt07_ppt08<- (dat_lightning$ppt06+ dat_lightning$ppt07 + dat_lightning$ppt08)/3
dat_lightning$mean_ppt07_ppt08_ppt09<- (dat_lightning$ppt07+ dat_lightning$ppt08 + dat_lightning$ppt09)/3
dat_lightning$mean_ppt05_ppt06_ppt07_ppt08<- (dat_lightning$ppt05+ dat_lightning$ppt06 + dat_lightning$ppt07 + dat_lightning$ppt08)/4
dat_lightning$mean_ppt06_ppt07_ppt08_ppt09<- (dat_lightning$ppt06+ dat_lightning$ppt07 + dat_lightning$ppt08 + dat_lightning$ppt09)/4
dat_lightning$mean_ppt05_ppt06_ppt07_ppt08_ppt09<- (dat_lightning$ppt05 + dat_lightning$ppt06 + dat_lightning$ppt07 + dat_lightning$ppt08 + dat_lightning$ppt09)/5

dat_lightning$mean_mdc05_mdc06<- (dat_lightning$mdc_05+ dat_lightning$mdc_06)/2
dat_lightning$mean_mdc06_mdc07<- (dat_lightning$mdc_06+ dat_lightning$mdc_07)/2
dat_lightning$mean_mdc07_mdc08<- (dat_lightning$mdc_07+ dat_lightning$mdc_08)/2
dat_lightning$mean_mdc08_mdc09<- (dat_lightning$mdc_08+ dat_lightning$mdc_09)/2
dat_lightning$mean_mdc05_mdc06_mdc07<- (dat_lightning$mdc_05+ dat_lightning$mdc_06 + dat_lightning$mdc_07)/3
dat_lightning$mean_mdc06_mdc07_mdc08<- (dat_lightning$mdc_06+ dat_lightning$mdc_07 + dat_lightning$mdc_08)/3
dat_lightning$mean_mdc07_mdc08_mdc09<- (dat_lightning$mdc_07+ dat_lightning$mdc_08 + dat_lightning$mdc_09)/3
dat_lightning$mean_mdc05_mdc06_mdc07_mdc08<- (dat_lightning$mdc_05+ dat_lightning$mdc_06 + dat_lightning$mdc_07 + dat_lightning$mdc_08)/4
dat_lightning$mean_mdc06_mdc07_mdc08_mdc09<- (dat_lightning$mdc_06+ dat_lightning$mdc_07 + dat_lightning$mdc_08 + dat_lightning$mdc_09)/4
dat_lightning$mean_mdc05_mdc06_mdc07_mdc08_mdc09<- (dat_lightning$mdc_05 + dat_lightning$mdc_06+ dat_lightning$mdc_07 + dat_lightning$mdc_08 + dat_lightning$mdc_09)/5


```

```{r}
#Climate variable creation for person-caused fire data

### creating amalgamations of variables to test different combinations of variables.##
dat_person$mean_tmax05_tmax06<- (dat_person$tmax05+ dat_person$tmax06)/2
dat_person$mean_tmax06_tmax07<- (dat_person$tmax06+ dat_person$tmax07)/2
dat_person$mean_tmax07_tmax08<- (dat_person$tmax07+ dat_person$tmax08)/2
dat_person$mean_tmax08_tmax09<- (dat_person$tmax08+ dat_person$tmax09)/2
dat_person$mean_tmax05_tmax06_tmax07<- (dat_person$tmax05+ dat_person$tmax06 + dat_person$tmax07)/3
dat_person$mean_tmax06_tmax07_tmax08<- (dat_person$tmax06+ dat_person$tmax07 + dat_person$tmax08)/3
dat_person$mean_tmax07_tmax08_tmax09<- (dat_person$tmax07+ dat_person$tmax08 + dat_person$tmax09)/3
dat_person$mean_tmax05_tmax06_tmax07_tmax08<- (dat_person$tmax05 + dat_person$tmax06+ dat_person$tmax07 + dat_person$tmax08)/4
dat_person$mean_tmax06_tmax07_tmax08_tmax09<- (dat_person$tmax06 + dat_person$tmax07+ dat_person$tmax08 + dat_person$tmax09)/4
dat_person$mean_tmax05_tmax06_tmax07_tmax08_tmax09<- (dat_person$tmax05 + dat_person$tmax06 + dat_person$tmax07+ dat_person$tmax08 + dat_person$tmax09)/5

dat_person$mean_tave05_tave06<- (dat_person$tave05+ dat_person$tave06)/2
dat_person$mean_tave06_tave07<- (dat_person$tave06+ dat_person$tave07)/2
dat_person$mean_tave07_tave08<- (dat_person$tave07+ dat_person$tave08)/2
dat_person$mean_tave08_tave09<- (dat_person$tave08+ dat_person$tave09)/2
dat_person$mean_tave05_tave06_tave07<- (dat_person$tave05+ dat_person$tave06 + dat_person$tave07)/3
dat_person$mean_tave06_tave07_tave08<- (dat_person$tave06+ dat_person$tave07 + dat_person$tave08)/3
dat_person$mean_tave07_tave08_tave09<- (dat_person$tave07+ dat_person$tave08 + dat_person$tave09)/3
dat_person$mean_tave05_tave06_tave07_tave08<- (dat_person$tave05 + dat_person$tave06+ dat_person$tave07 + dat_person$tave08)/4
dat_person$mean_tave06_tave07_tave08_tave09<- (dat_person$tave06 + dat_person$tave07+ dat_person$tave08 + dat_person$tave09)/4
dat_person$mean_tave05_tave06_tave07_tave08_tave09<- (dat_person$tave05 + dat_person$tave06 + dat_person$tave07+ dat_person$tave08 + dat_person$tave09)/5


dat_person$mean_ppt05_ppt06<- (dat_person$ppt05+ dat_person$ppt06)/2
dat_person$mean_ppt06_ppt07<- (dat_person$ppt06+ dat_person$ppt07)/2
dat_person$mean_ppt07_ppt08<- (dat_person$ppt07+ dat_person$ppt08)/2
dat_person$mean_ppt08_ppt09<- (dat_person$ppt08+ dat_person$ppt09)/2
dat_person$mean_ppt05_ppt06_ppt07<- (dat_person$ppt05+ dat_person$ppt06 + dat_person$ppt07)/3
dat_person$mean_ppt06_ppt07_ppt08<- (dat_person$ppt06+ dat_person$ppt07 + dat_person$ppt08)/3
dat_person$mean_ppt07_ppt08_ppt09<- (dat_person$ppt07+ dat_person$ppt08 + dat_person$ppt09)/3
dat_person$mean_ppt05_ppt06_ppt07_ppt08<- (dat_person$ppt05+ dat_person$ppt06 + dat_person$ppt07 + dat_person$ppt08)/4
dat_person$mean_ppt06_ppt07_ppt08_ppt09<- (dat_person$ppt06+ dat_person$ppt07 + dat_person$ppt08 + dat_person$ppt09)/4
dat_person$mean_ppt05_ppt06_ppt07_ppt08_ppt09<- (dat_person$ppt05 + dat_person$ppt06 + dat_person$ppt07 + dat_person$ppt08 + dat_person$ppt09)/5

dat_person$mean_mdc05_mdc06<- (dat_person$mdc_05+ dat_person$mdc_06)/2
dat_person$mean_mdc06_mdc07<- (dat_person$mdc_06+ dat_person$mdc_07)/2
dat_person$mean_mdc07_mdc08<- (dat_person$mdc_07+ dat_person$mdc_08)/2
dat_person$mean_mdc08_mdc09<- (dat_person$mdc_08+ dat_person$mdc_09)/2
dat_person$mean_mdc05_mdc06_mdc07<- (dat_person$mdc_05+ dat_person$mdc_06 + dat_person$mdc_07)/3
dat_person$mean_mdc06_mdc07_mdc08<- (dat_person$mdc_06+ dat_person$mdc_07 + dat_person$mdc_08)/3
dat_person$mean_mdc07_mdc08_mdc09<- (dat_person$mdc_07+ dat_person$mdc_08 + dat_person$mdc_09)/3
dat_person$mean_mdc05_mdc06_mdc07_mdc08<- (dat_person$mdc_05+ dat_person$mdc_06 + dat_person$mdc_07 + dat_person$mdc_08)/4
dat_person$mean_mdc06_mdc07_mdc08_mdc09<- (dat_person$mdc_06+ dat_person$mdc_07 + dat_person$mdc_08 + dat_person$mdc_09)/4
dat_person$mean_mdc05_mdc06_mdc07_mdc08_mdc09<- (dat_person$mdc_05 + dat_person$mdc_06+ dat_person$mdc_07 + dat_person$mdc_08 + dat_person$mdc_09)/5
```

Create variable groupings to put into model selection loop.

```{r}

variables<- c("tmax05","tmax06", "tmax07", "tmax08", "tmax09", 
              "mean_tmax05_tmax06","mean_tmax06_tmax07", "mean_tmax07_tmax08", "mean_tmax08_tmax09", 
              "mean_tmax05_tmax06_tmax07", "mean_tmax06_tmax07_tmax08","mean_tmax07_tmax08_tmax09", 
              "mean_tmax05_tmax06_tmax07_tmax08", "mean_tmax06_tmax07_tmax08_tmax09", "mean_tmax05_tmax06_tmax07_tmax08_tmax09",
              
              "tave05","tave06", "tave07", "tave08", "tave09", 
              "mean_tave05_tave06","mean_tave06_tave07", "mean_tave07_tave08", "mean_tave08_tave09", 
              "mean_tave05_tave06_tave07", "mean_tave06_tave07_tave08","mean_tave07_tave08_tave09", 
              "mean_tave05_tave06_tave07_tave08", "mean_tave06_tave07_tave08_tave09", "mean_tave05_tave06_tave07_tave08_tave09",
              
              "ppt05","ppt06", "ppt07", "ppt08", "ppt09",
              "mean_ppt05_ppt06", "mean_ppt06_ppt07", "mean_ppt07_ppt08", "mean_ppt08_ppt09", 
              "mean_ppt05_ppt06_ppt07","mean_ppt06_ppt07_ppt08", "mean_ppt07_ppt08_ppt09",
              "mean_ppt05_ppt06_ppt07_ppt08", "mean_ppt06_ppt07_ppt08_ppt09",
              "mean_ppt05_ppt06_ppt07_ppt08_ppt09",
              
              "mdc_05","mdc_06", "mdc_07", "mdc_08", "mdc_09",
              "mean_mdc05_mdc06", "mean_mdc06_mdc07", "mean_mdc07_mdc08", "mean_mdc08_mdc09", 
              "mean_mdc05_mdc06_mdc07", "mean_mdc06_mdc07_mdc08", "mean_mdc07_mdc08_mdc09", 
              "mean_mdc05_mdc06_mdc07_mdc08", "mean_mdc06_mdc07_mdc08_mdc09",
              "mean_mdc05_mdc06_mdc07_mdc08_mdc09")

 variables1<-c("tmax05", "tmax06", "tmax07", "tmax08", "tmax09",
               "tave05", "tave06", "tave07", "tave08", "tave09"
#               "tmax05","tmax06", "tmax07", "tmax08", "tmax09",
#               "mdc_05", "mdc_06", "mdc_07", "mdc_08", "mdc_09"
)
variables2<-c("ppt05", "ppt06", "ppt07", "ppt08", "ppt09",
              "ppt05", "ppt06", "ppt07", "ppt08", "ppt09"
              # "mdc_05", "mdc_06", "mdc_07", "mdc_08", "mdc_09",
              # "ppt05", "ppt06", "ppt07", "ppt08", "ppt09"
) 
# precipitation and MDC and temperature and MDC are quite correlated so I'm leaving this combination of variables out. 

```

Modify variable names for ease in analysis.

```{r}
#Lightning
dat_lightning$fire_pres<-as.numeric(dat_lightning$fire) 
table(dat_lightning$fire_yr, dat_lightning$fire_pres)
table(dat_lightning$fire_yr, dat_lightning$fire_cs, dat_lightning$zone)

#Person-caused
dat_person$fire_pres<-as.numeric(dat_person$fire) 
table(dat_person$fire_yr, dat_person$fire_pres)
table(dat_person$fire_yr, dat_person$fire_cs, dat_person$zone)

```

#################################
#### Running simple logistic regression model
#################################

Now that the data is prepped, we are ready to run the logistic regression model for the lightning and person caused fires separately to determine the best climate variables.

```{r}
# create loop to do variable selection of climate data
unique(dat_lightning$zone)
zones<- c("ICH", "ESSF", "CWH", "MH", "CMA", "MS", "PP", "IDF", "SBPS", "IMA", "BWBS", "BG", "SBS", "SWB") #"CDF", "BAFA"

# CDF and BAFA have few fire ignitions (CHECK!), I'm going to leave them out for the moment because there are not many fire ignition locations in these two. Other BEC zones that may need to be excluded due to small sample sizes include: SWB, CMA, and maybe BG. IMA seems to have small sample sizes, but consistent results.

filenames<-list()
prop<-0.75

```

Model selection for lightning caused fires:

```{r}

for (g in 1:100){

for (h in 1:length(zones)) {
  dat2<- dat_lightning %>% dplyr::filter(zone ==zones[h])
  
#Create frame of AIC table
# summary table
table.glm.climate.simple <- data.frame (matrix (ncol = 4, nrow = 0))
colnames (table.glm.climate.simple) <- c ("Zone", "Variable", "AIC", "AUC")

model_dat<- dat2 %>% dplyr::select(fire_pres)
trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                  list = FALSE,
                                  times = 1)
dat1 <- as.data.frame(model_dat[ trainIndex,])
names(dat1)[1] <- "fire_pres"
Valid <- as.data.frame(model_dat[-trainIndex,])
names(Valid)[1] <- "fire_pres"


model1 <- glm (fire_pres ~ 1 ,
               data=dat1,
               family = binomial (link = "logit"))

table.glm.climate.simple[1,1]<-zones[h]
table.glm.climate.simple[1,2]<-"intercept"
table.glm.climate.simple[1,3]<-extractAIC(model1)[2]

# lets look at fit of the Valid (validation) dataset
Valid$model1_predict <- predict.glm(model1,newdata = Valid,type="response")
roc_obj <- roc(Valid$fire_pres, Valid$model1_predict)
auc(roc_obj)
table.glm.climate.simple[1,4]<-auc(roc_obj)

rm(model_dat,dat1,Valid)

for (i in 1: length(variables)){
  print(paste((variables[i]), (zones[h]), sep=" "))
  
  model_dat<- dat2 %>% dplyr::select(fire_pres, variables[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]
  
  model1 <- glm (fire_pres ~ . ,
                 data=dat1,
                 family = binomial (link = "logit"))
  
  table.glm.climate.simple[i+1,1]<-zones[h]
  table.glm.climate.simple[i+1,2]<-variables[i]
  table.glm.climate.simple[i+1,3]<-extractAIC(model1)[2]
  
  # lets look at fit of the Valid (validation) dataset
  Valid$model1_predict <- predict.glm(model1,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model1_predict)
  auc(roc_obj)
  table.glm.climate.simple[i+1,4]<-auc(roc_obj)
  
}

# This is an addition to the table above allowing combinations of temperature and precipitation

for (i in 1: length(variables1)){
  print(paste((variables1[i]), variables2[i], (zones[h]), sep=" "))
  model_dat<- dat2 %>% dplyr::select(fire_pres, variables1[i], variables2[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]
  
  model2 <- glm (fire_pres ~ . ,
                 data=dat1,
                 family = binomial (link = "logit"))
  
  table.glm.climate.simple[(i+length(variables))+1,1]<-zones[h]
  table.glm.climate.simple[(i+length(variables))+1,2]<-paste0(variables1[i],"+", variables2[i])
  table.glm.climate.simple[(i+length(variables))+1,3]<-extractAIC(model2)[2]
  
  Valid$model2_predict <- predict.glm(model2,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model2_predict)
  auc(roc_obj)
  table.glm.climate.simple[(i+length(variables))+1,4]<-auc(roc_obj)
  
}

for (i in 1: length(variables1)){
  print(paste((variables1[i]), "x",variables2[i], (zones[h]), sep=" "))

  model_dat<- dat2 %>% dplyr::select(fire_pres, variables1[i], variables2[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]

  model2 <- glm (fire_pres ~ (.)^2,
                 data=dat1,
                 family = binomial (link = "logit"))

  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),1]<-zones[h]
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),2]<-paste0(variables1[i],"x", variables2[i])
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),3]<-extractAIC(model2)[2]

  Valid$model2_predict <- predict.glm(model2,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model2_predict)
  auc(roc_obj)
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),4]<-auc(roc_obj)

}
table.glm.climate1<-table.glm.climate.simple %>% drop_na(AIC)


#assign file names to the work
nam1<-paste("AIC",zones[h],"run",g,sep="_") #defining the name
assign(nam1,table.glm.climate.simple)
filenames<-append(filenames,nam1)
}
}
# Common warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred


```

Save output:

```{r}

mkFrameList <- function(nfiles) {
  d <- lapply(seq_len(nfiles),function(i) {
    eval(parse(text=filenames[i]))
  })
  do.call(rbind,d)
}

n<-length(filenames)
aic_bec_lightning<-mkFrameList(n) 

aic_bec_lightning_summary<- aic_bec_lightning %>%
  group_by(Zone, Variable) %>%
  summarise(meanAIC=mean(AIC),
            meanAUC=mean(AUC),
            sdAUC=sd(AUC),
            )

aic_bec_lightning_summary2<- aic_bec_lightning_summary %>%
  group_by(Zone) %>%
  mutate(deltaAIC=meanAIC-min(meanAIC))

aic_bec_lightning_summary2

```

Save file to local computer:

```{r}
write.csv(aic_bec_lightning_summary2, file="D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\climate_AIC_results_lightning_July8.csv")
```


Repeat for person-caused fires:

```{r}
##Different zones need to be excluded
unique(dat_person$zone)

zones<- c("SBS", "ICH", "ESSF", "MH",  "CWH", "CMA", "SBPS", "MS", "PP", "IDF", "BWBS","BG", "CDF") #"BAFA", "IMA", "SWB"


filenames<-list()
prop<-0.75

##Run loop
for (g in 1:100){

for (h in 1:length(zones)) {
  dat2<- dat_person %>% dplyr::filter(zone ==zones[h])
  
#Create frame of AIC table
# summary table
table.glm.climate.simple <- data.frame (matrix (ncol = 4, nrow = 0))
colnames (table.glm.climate.simple) <- c ("Zone", "Variable", "AIC", "AUC")

model_dat<- dat2 %>% dplyr::select(fire_pres)
trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                  list = FALSE,
                                  times = 1)
dat1 <- as.data.frame(model_dat[ trainIndex,])
names(dat1)[1] <- "fire_pres"
Valid <- as.data.frame(model_dat[-trainIndex,])
names(Valid)[1] <- "fire_pres"


model1 <- glm (fire_pres ~ 1 ,
               data=dat1,
               family = binomial (link = "logit"))

table.glm.climate.simple[1,1]<-zones[h]
table.glm.climate.simple[1,2]<-"intercept"
table.glm.climate.simple[1,3]<-extractAIC(model1)[2]

# lets look at fit of the Valid (validation) dataset
Valid$model1_predict <- predict.glm(model1,newdata = Valid,type="response")
roc_obj <- roc(Valid$fire_pres, Valid$model1_predict)
auc(roc_obj)
table.glm.climate.simple[1,4]<-auc(roc_obj)

rm(model_dat,dat1,Valid)

for (i in 1: length(variables)){
  print(paste((variables[i]), (zones[h]), sep=" "))
  
  model_dat<- dat2 %>% dplyr::select(fire_pres, variables[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]
  
  model1 <- glm (fire_pres ~ . ,
                 data=dat1,
                 family = binomial (link = "logit"))
  
  table.glm.climate.simple[i+1,1]<-zones[h]
  table.glm.climate.simple[i+1,2]<-variables[i]
  table.glm.climate.simple[i+1,3]<-extractAIC(model1)[2]
  
  # lets look at fit of the Valid (validation) dataset
  Valid$model1_predict <- predict.glm(model1,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model1_predict)
  auc(roc_obj)
  table.glm.climate.simple[i+1,4]<-auc(roc_obj)
  
}

# This is an addition to the table above allowing combinations of temperature and precipitation

for (i in 1: length(variables1)){
  print(paste((variables1[i]), variables2[i], (zones[h]), sep=" "))
  model_dat<- dat2 %>% dplyr::select(fire_pres, variables1[i], variables2[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]
  
  model2 <- glm (fire_pres ~ . ,
                 data=dat1,
                 family = binomial (link = "logit"))
  
  table.glm.climate.simple[(i+length(variables))+1,1]<-zones[h]
  table.glm.climate.simple[(i+length(variables))+1,2]<-paste0(variables1[i],"+", variables2[i])
  table.glm.climate.simple[(i+length(variables))+1,3]<-extractAIC(model2)[2]
  
  Valid$model2_predict <- predict.glm(model2,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model2_predict)
  auc(roc_obj)
  table.glm.climate.simple[(i+length(variables))+1,4]<-auc(roc_obj)
  
}

for (i in 1: length(variables1)){
  print(paste((variables1[i]), "x",variables2[i], (zones[h]), sep=" "))

  model_dat<- dat2 %>% dplyr::select(fire_pres, variables1[i], variables2[i])
  # Creating training and testing datasets so that I can get a measure of how well the model actually predicts the data e.g. AUG
  trainIndex <- createDataPartition(model_dat$fire_pres, p = prop,
                                    list = FALSE,
                                    times = 1)
  dat1 <- model_dat[ trainIndex,]
  Valid <- model_dat[-trainIndex,]

  model2 <- glm (fire_pres ~ (.)^2,
                 data=dat1,
                 family = binomial (link = "logit"))

  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),1]<-zones[h]
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),2]<-paste0(variables1[i],"x", variables2[i])
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),3]<-extractAIC(model2)[2]

  Valid$model2_predict <- predict.glm(model2,newdata = Valid,type="response")
  roc_obj <- roc(Valid$fire_pres, Valid$model2_predict)
  auc(roc_obj)
  table.glm.climate.simple[(i+length(variables) +length(variables1) + 1),4]<-auc(roc_obj)

}
table.glm.climate1<-table.glm.climate.simple %>% drop_na(AIC)


#assign file names to the work
nam1<-paste("AIC",zones[h],"run",g,sep="_") #defining the name
assign(nam1,table.glm.climate.simple)
filenames<-append(filenames,nam1)
}
}


```

Save output:

```{r}

mkFrameList <- function(nfiles) {
  d <- lapply(seq_len(nfiles),function(i) {
    eval(parse(text=filenames[i]))
  })
  do.call(rbind,d)
}

n<-length(filenames)
aic_bec_person<-mkFrameList(n) 

aic_bec_person_summary<- aic_bec_person %>%
  group_by(Zone, Variable) %>%
  summarise(meanAIC=mean(AIC),
            meanAUC=mean(AUC),
            sdAUC=sd(AUC),
            )

aic_bec_person_summary2<- aic_bec_person_summary %>%
  group_by(Zone) %>%
  mutate(deltaAIC=meanAIC-min(meanAIC))

head(aic_bec_person_summary2)
```

Save file to local computer:

```{r}
write.csv(aic_bec_person_summary2, file="D:\\Fire\\fire_data\\raw_data\\ClimateBC_Data\\climate_AIC_results_person_July9b.csv")

```

Save lightning and person caused fire datasets to clus.
#Below is not working

```{r}

###
connKyle <- dbConnect(drv = RPostgreSQL::PostgreSQL(), 
                      host = key_get('dbhost', keyring = 'postgreSQL'),
                      user = key_get('dbuser', keyring = 'postgreSQL'),
                      dbname = key_get('dbname', keyring = 'postgreSQL'),
                      password = key_get('dbpass', keyring = 'postgreSQL'),
                      port = "5432")
st_write (obj = dat_lightning, 
          dsn = connKyle, 
          layer = c ("public", "Data_Lightning"))
dbDisconnect (connKyle)
```

```{r}
connKyle <- dbConnect(drv = RPostgreSQL::PostgreSQL(), 
                      host = key_get('dbhost', keyring = 'postgreSQL'),
                      user = key_get('dbuser', keyring = 'postgreSQL'),
                      dbname = key_get('dbname', keyring = 'postgreSQL'),
                      password = key_get('dbpass', keyring = 'postgreSQL'),
                      port = "5432")
st_write (obj = dat_person, 
          dsn = connKyle, 
          layer = c ("public", "Data_Person"))
dbDisconnect (connKyle)
```


Clear work space to help R run faster again


```{r}
library(gdata)

keep(fire_veg_data_B, ignition_pres_abs4, fire_veg_data_lightning_5x, fire_veg_data_person_5x, fire_veg_data_NA_5x, fire_veg_data_lightning_NA_5x, fire_veg_data_person_NA_5x, dat_lightning, dat_person, aic_bec_person_summary2, aic_bec_lightning_summary2, sure = TRUE) # setting sure to TRUE removes variables not in the list
 
ls()
gc(TRUE)
```


############### Complete. Now move on to 06_Fire_ignition_model_fits_by_BEC wherein the results of these AIC tests will be utilized###########

Note: for person-caused fires, we still need to load the roads map and decide whether to use the roads clus to predict road occurence back in time to 2002, or keep as is.
